<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Maximum likelihood estimation | Jessica L. Zhou </title> <meta name="author" content="Jessica L. Zhou"> <meta name="description" content="A Python-coded implementation of maximum likelihood estimation for recovering the parameters used to define statistical distributions by fitting to values sampled from the distribution"> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://zrcjessica.github.io/blog/2023/mle/"> <script src="/assets/js/theme.js?9a0c749ec5240d9cda97bc72359a72c0"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>initTheme();</script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> <span class="font-weight-bold">Jessica</span> L. Zhou </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about </a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/">blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/photos/">photos </a> </li> <li class="nav-item "> <a class="nav-link" href="/repositories/">repositories </a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">cv </a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">Maximum likelihood estimation</h1> <p class="post-meta"> Created in February 21, 2023 </p> <p class="post-tags"> <a href="/blog/2023"> <i class="fa-solid fa-calendar fa-sm"></i> 2023 </a>   ·   <a href="/blog/tag/statistics"> <i class="fa-solid fa-hashtag fa-sm"></i> statistics</a>   <a href="/blog/tag/mle"> <i class="fa-solid fa-hashtag fa-sm"></i> mle</a>   <a href="/blog/tag/machine-learning"> <i class="fa-solid fa-hashtag fa-sm"></i> machine_learning</a>   <a href="/blog/tag/code"> <i class="fa-solid fa-hashtag fa-sm"></i> code</a>   <a href="/blog/tag/jupyter"> <i class="fa-solid fa-hashtag fa-sm"></i> jupyter</a>   ·   <a href="/blog/category/tutorials"> <i class="fa-solid fa-tag fa-sm"></i> tutorials</a>   <a href="/blog/category/code"> <i class="fa-solid fa-tag fa-sm"></i> code</a> </p> </header> <article class="post-content"> <div id="markdown-content"> <p>Maximum likelihood estimation (MLE) is a method used for estimating the parameters used to define a probability distribution by fitting to observed data drawn from said distribution. As the name implies, this method involves maximizing the <em>likelihood</em> that the observed data comes from a statistical model. A silly analogy I sometimes use to explain MLE involves finding clothes that fit. Here, the observed data is someone who’s trying to find clothes that fit. MLE is like the process of trying on clothing items of different sizes until we find the sizes that fit the best. The clothing sizes are analogous to the parameters that define the shape of statistical distribution from which the observed data came (so the distribution itself is equivalent to the piece of clothing). Assuming the individual has no idea what their size is (this is actually somewhat realistic for womens fashion, where sizing can be wildly inconsistent across brands), MLE will start with a set of guesses at what the best sizes could be and then try different sizes to improve the fit over each iteration. In this tutorial, we will simulate data and use MLE to recover the parameters used to simulate the data. A Jupyter notebook for this tutorial can be found on my <a href="https://github.com/zrcjessica/ml_concepts/blob/main/MLE_tutorial.ipynb" rel="external nofollow noopener" target="_blank">GitHub</a>.</p> <h1 id="mle-for-normally-distributed-data">MLE for normally distributed data</h1> <p>Let’s begin by walking through an example using MLE to estimate the parameters for a <a href="https://en.wikipedia.org/wiki/Normal_distribution" rel="external nofollow noopener" target="_blank">normal distribution</a> from which we draw a sample set of observed data. To begin, let’s start by drawing $N=1000$ data points from a normal distribution defined as \(N(\mu=4,\sigma=3)\). We will refer to the sampled data as \(\mathbf{x} = x_1, x_2,...,x_N\).</p> <h2 id="simulate-data">Simulate data</h2> <figure class="highlight"><pre><code class="language-python" data-lang="python"><table class="rouge-table"><tbody><tr>
<td class="gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
</pre></td> <td class="code"><pre> <span class="c1"># load the packages we'll need for this tutorial
</span> <span class="kn">import</span> <span class="n">numpy</span> <span class="k">as</span> <span class="n">np</span>
 <span class="kn">from</span> <span class="n">scipy</span> <span class="kn">import</span> <span class="n">optimize</span>
 <span class="kn">from</span> <span class="n">scipy</span> <span class="kn">import</span> <span class="n">stats</span> 
 <span class="kn">import</span> <span class="n">matplotlib.pyplot</span> <span class="k">as</span> <span class="n">plt</span>
 <span class="kn">import</span> <span class="n">math</span>
 <span class="kn">from</span> <span class="n">scipy.special</span> <span class="kn">import</span> <span class="n">factorial</span>
 
 <span class="c1"># define parameters
</span> <span class="n">MU</span> <span class="o">=</span> <span class="mi">4</span>
 <span class="n">SIGMA</span> <span class="o">=</span> <span class="mi">3</span>
 <span class="n">N</span> <span class="o">=</span> <span class="mi">1000</span>
 
 <span class="c1"># add some noise
</span> <span class="n">noise</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">randn</span><span class="p">(</span><span class="n">N</span><span class="p">)</span>
 
 <span class="c1"># sample data from normal distribution
</span> <span class="n">norm_sample</span> <span class="o">=</span> <span class="n">stats</span><span class="p">.</span><span class="n">norm</span><span class="p">.</span><span class="nf">rvs</span><span class="p">(</span><span class="n">loc</span> <span class="o">=</span> <span class="n">MU</span><span class="p">,</span> <span class="n">scale</span> <span class="o">=</span> <span class="n">SIGMA</span><span class="p">,</span> <span class="n">size</span> <span class="o">=</span> <span class="n">N</span><span class="p">)</span> <span class="o">+</span> <span class="n">noise</span>
</pre></td> </tr></tbody></table></code></pre></figure> <h3 id="visualize">Visualize</h3> <p>Let’s take a look at what our sample data looks like.</p> <figure class="highlight"><pre><code class="language-python" data-lang="python"><table class="rouge-table"><tbody><tr>
<td class="gutter gl"><pre class="lineno">1
2
3
4
</pre></td> <td class="code"><pre> <span class="n">plt</span><span class="p">.</span><span class="nf">hist</span><span class="p">(</span><span class="n">norm_sample</span><span class="p">)</span>
 <span class="n">plt</span><span class="p">.</span><span class="nf">xlabel</span><span class="p">(</span><span class="sh">'</span><span class="s">x</span><span class="sh">'</span><span class="p">)</span>
 <span class="n">plt</span><span class="p">.</span><span class="nf">title</span><span class="p">(</span><span class="sh">'</span><span class="s">distribution of Gaussian random sample</span><span class="sh">'</span><span class="p">)</span>
 <span class="n">plt</span><span class="p">.</span><span class="nf">show</span><span class="p">()</span>
</pre></td> </tr></tbody></table></code></pre></figure> <p>You should get something that looks like this:</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/mle/guassian_sample.png" sizes="95vw"></source> <img src="/assets/img/mle/guassian_sample.png" class="img-fluid" width="400" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <h2 id="define-likelihood-function">Define likelihood function</h2> <p>In MLE, we want to optimize a <strong>likelihood function</strong>, which measures the likelihood that a given set of data came from a statistical distribution defined by a current set of parameters. The <a href="https://en.wikipedia.org/wiki/Maximum_likelihood_estimation" rel="external nofollow noopener" target="_blank">likelihood function (\(\mathcal{L}\) for a continuous distribution</a> (our data is continuous) is defined as:</p> \[\mathcal{L}(\theta,\mathbf{x}) = f_{\theta}(\mathbf{x})=P_{\theta}(X=\mathbf{x})\] <p>Here, \(\theta={\theta_1,...,\theta_M}\) represents the set of \(M\) parameters defining the distribution that we are trying to fit the data to. \(f_{\theta}(\mathbf{x})=P_{\theta}(X=\mathbf{x})\) is just another way of writing the probability density function (PDF) for the continuous distribution given the parameterization \(\theta\). In other words, it means “what is the probability that the data we are observing came from the statistical distribution defined the parameters \(\theta\)?” Now because in MLE we are typically looking at a set of data points, we are actually trying to find the <em>joint probability</em> that all of the data points in our sample came from the same distribution defined by a given set of parameters \(\theta\). Because the joint probability of <em>identical and independently distributed</em> variables is simply their product, our likelihood function can therefore be expressed as:</p> \[\mathcal{L}(\theta,\mathbf{x}) = \prod _{i=1} ^{N} f_{\theta}(\mathbf{x})= \prod _{i=1} ^{N} P_{\theta}(X=\mathbf{x})\] <p>In our example, we will try to fit the data back to a normal distribution. Therefore, \(\theta=[\mu,\sigma]\). Even if we didn’t have the prior knowledge that we sampled our data from a normal distribution, observing the distribution of the data clearly indicates that the data follows a normal distribution, making it a good distribution to fit our data to. The PDF for the normal distribution is defined as:</p> \[f_{\mu,\sigma}(x) = f(x|\mu,\sigma) = \frac{1}{\sqrt{2\pi\sigma^2}} \exp { \left( - \frac{(x-\mu)^2}{2\sigma^2} \right) }\] <p>This is the function that we will be trying to maximize. However, in practice it is more common to optimize over the <strong>log-likelihood</strong> function (LLF), because the product of many probability values can lead to floating point errors in computation. The LLF can be expressed as:</p> \[\log{\left( \mathcal{L}(\theta | \mathbf{x}) \right)} = \sum _{i=1} ^N \log{ \left( f_{\theta}(x_i) \right)} = \sum _{i=1} ^N \log{(P_{\theta})}(X=x_i)\] <h2 id="optimize">Optimize</h2> <p>Many built-in optimizers don’t have a maximization function, just a minimization function. Therefore, instead of maximizing the log-likelihood, we will <strong>minimize the negative log-likelihood</strong>.</p> <figure class="highlight"><pre><code class="language-python" data-lang="python"><table class="rouge-table"><tbody><tr>
<td class="gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
</pre></td> <td class="code"><pre> <span class="c1"># define neg llf
</span> <span class="k">def</span> <span class="nf">ll_norm</span><span class="p">(</span><span class="n">params</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
     <span class="sh">'''</span><span class="s">
     params is a list of parameters to estimate: [mu, sigma]
     x is list of normally distributed values described by estimated params
     </span><span class="sh">'''</span>
    
     <span class="n">mu</span> <span class="o">=</span> <span class="n">params</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
     <span class="n">sigma</span> <span class="o">=</span> <span class="n">params</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
     <span class="n">loglik</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">log</span><span class="p">((</span><span class="mi">1</span><span class="o">/</span><span class="p">((</span><span class="mi">2</span><span class="o">*</span><span class="n">math</span><span class="p">.</span><span class="n">pi</span><span class="o">*</span><span class="n">sigma</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span><span class="o">**</span><span class="mf">0.5</span><span class="p">))</span><span class="o">*</span><span class="n">np</span><span class="p">.</span><span class="nf">exp</span><span class="p">(</span><span class="o">-</span><span class="p">((</span><span class="n">x</span><span class="o">-</span><span class="n">mu</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span><span class="o">/</span><span class="p">(</span><span class="mi">2</span><span class="o">*</span><span class="n">sigma</span><span class="o">**</span><span class="mi">2</span><span class="p">)))</span>
     <span class="k">return</span> <span class="o">-</span><span class="n">loglik</span><span class="p">.</span><span class="nf">sum</span><span class="p">()</span>
</pre></td> </tr></tbody></table></code></pre></figure> <p>Here’s a quick intro to essential arguments for the <a href="https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.minimize.html" rel="external nofollow noopener" target="_blank"><code class="language-plaintext highlighter-rouge">scipy.optimize.minimize</code></a> function: -<code class="language-plaintext highlighter-rouge">fun</code>: function to minimize - for us this is the negative log-likelihood function</p> <ul> <li> <code class="language-plaintext highlighter-rouge">x0</code>: initia guesses for the parameters we’re estimating, in the form of a list, tuple, or numpy array</li> <li> <code class="language-plaintext highlighter-rouge">args</code>: any other variables to pass to the function <code class="language-plaintext highlighter-rouge">fun</code>, given in a list, tuple, or numpy array</li> <li> <code class="language-plaintext highlighter-rouge">bounds</code>: bounds for parameters to be estimated, given as a list of lists or tuple of tuples, corresponds to params in <code class="language-plaintext highlighter-rouge">x0</code> </li> </ul> <figure class="highlight"><pre><code class="language-python" data-lang="python"><table class="rouge-table"><tbody><tr>
<td class="gutter gl"><pre class="lineno">1
2
3
4
5
6
7
</pre></td> <td class="code"><pre><span class="c1"># minimize negative log-likelihood
</span> <span class="n">norm_res</span> <span class="o">=</span> <span class="n">optimize</span><span class="p">.</span><span class="nf">minimize</span><span class="p">(</span><span class="n">fun</span> <span class="o">=</span> <span class="n">ll_norm</span><span class="p">,</span>
               <span class="n">x0</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">],</span>
               <span class="n">args</span> <span class="o">=</span> <span class="p">(</span><span class="n">norm_sample</span><span class="p">),</span>
             <span class="n">bounds</span> <span class="o">=</span> <span class="p">((</span><span class="bp">None</span><span class="p">,</span><span class="bp">None</span><span class="p">),(</span><span class="mf">1e-6</span><span class="p">,</span><span class="bp">None</span><span class="p">)))</span>

 <span class="n">norm_res</span>
</pre></td> </tr></tbody></table></code></pre></figure> <p>The output should look something like this:</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>      fun: 2509.8651698184385
 hess_inv: &lt;2x2 LbfgsInvHessProduct with dtype=float64&gt;
      jac: array([ 0.00040927, -0.00027285])
  message: b'CONVERGENCE: REL_REDUCTION_OF_F_&lt;=_FACTR*EPSMCH'
     nfev: 45
      nit: 14
   status: 0
  success: True
        x: array([4.02069038, 2.97703045])

</code></pre></div></div> <p>Here’s a brief overview of what these outputs mean:</p> <ul> <li> <code class="language-plaintext highlighter-rouge">fun</code>: minimum value of function at estimated parameters</li> <li> <code class="language-plaintext highlighter-rouge">nfev</code>: number of function evaluations</li> <li> <code class="language-plaintext highlighter-rouge">nit</code>: number of iterations</li> <li> <code class="language-plaintext highlighter-rouge">success</code>: bool - did the optimizer run into an issue?</li> <li> <code class="language-plaintext highlighter-rouge">x</code>: array of estimated parameters that minimize the function, corresponding to <code class="language-plaintext highlighter-rouge">x0</code> </li> </ul> <p>We can print the final parameter estimates from MLE and see how they compare to the values we used to simulate the data that we showed the algorithm:</p> <figure class="highlight"><pre><code class="language-python" data-lang="python"><table class="rouge-table"><tbody><tr>
<td class="gutter gl"><pre class="lineno">1
2
</pre></td> <td class="code"><pre> <span class="nf">print</span><span class="p">(</span><span class="sh">'</span><span class="s">mu = %d, sigma = %d</span><span class="sh">'</span> <span class="o">%</span> <span class="p">(</span><span class="n">MU</span><span class="p">,</span> <span class="n">SIGMA</span><span class="p">))</span>
 <span class="nf">print</span><span class="p">(</span><span class="sh">'</span><span class="s">mu_est = %.4f, sigma_est = %.4f</span><span class="sh">'</span> <span class="o">%</span> <span class="p">(</span><span class="n">norm_res</span><span class="p">.</span><span class="n">x</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">norm_res</span><span class="p">.</span><span class="n">x</span><span class="p">[</span><span class="mi">1</span><span class="p">]))</span>
</pre></td> </tr></tbody></table></code></pre></figure> <p>You should see that the estimated parameters \(\hat{\mu}\) and \(\hat{\sigma}\) are quite close to the true parameter values that we used to define the normal distribution from which we simulated the data points. If you refer to my <a href="https://github.com/zrcjessica/ml_concepts/blob/main/MLE_tutorial.ipynb" rel="external nofollow noopener" target="_blank">Jupyter notebook</a>, you can also see how changing the size of the data that we give to the MLE algorithm change the resulting parameter estimates.</p> <h1 id="mle-for-poisson-distributed-data">MLE for Poisson-distributed data</h1> <p>Now let’s see how we can use MLE to estimate the parameters of the distribution from which a set of <a href="https://en.wikipedia.org/wiki/Poisson_distribution" rel="external nofollow noopener" target="_blank">Poisson distributed data</a> came from. Poisson-distributed data is discrete and nonnegative. The Poisson distribution takes only one parameter, $\lambda&gt;0$, which represents the rate at which events occur.</p> <p>It is a discrete probability distribution that expresses the probability of a given number of events, \(k\), occurring in a fixed interval of time or space, if the events occur with a known constant rate \(\lambda\) and independently of the time since the last event.</p> <p>Here, we will randomly generate a set of Poisson distributed data for a specified value of \(\lambda\).</p> <p>Note: The <a href="https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.poisson.html" rel="external nofollow noopener" target="_blank">scipy notation for the Poisson distribution</a> uses \(\mu\) in place of \(\lambda\).</p> <h2 id="simulate-data-1">Simulate data</h2> <figure class="highlight"><pre><code class="language-python" data-lang="python"><table class="rouge-table"><tbody><tr>
<td class="gutter gl"><pre class="lineno">1
2
3
4
5
6
</pre></td> <td class="code"><pre> <span class="c1"># define parameters
</span> <span class="n">LAMBDA</span> <span class="o">=</span> <span class="mf">0.5</span>
 <span class="n">N</span> <span class="o">=</span> <span class="mi">1000</span>
 
 <span class="c1"># sample data from poisson distribution
</span> <span class="n">pois_sample</span> <span class="o">=</span> <span class="n">stats</span><span class="p">.</span><span class="n">poisson</span><span class="p">.</span><span class="nf">rvs</span><span class="p">(</span><span class="n">loc</span> <span class="o">=</span> <span class="n">MU</span><span class="p">,</span> <span class="n">scale</span> <span class="o">=</span> <span class="n">SIGMA</span><span class="p">,</span> <span class="n">size</span> <span class="o">=</span> <span class="n">N</span><span class="p">)</span>  
</pre></td> </tr></tbody></table></code></pre></figure> <h3 id="visualize-1">Visualize</h3> <p>Let’s take a look at what our sample data looks like.</p> <figure class="highlight"><pre><code class="language-python" data-lang="python"><table class="rouge-table"><tbody><tr>
<td class="gutter gl"><pre class="lineno">1
2
3
4
</pre></td> <td class="code"><pre> <span class="n">plt</span><span class="p">.</span><span class="nf">hist</span><span class="p">(</span><span class="n">pois_sample</span><span class="p">)</span>
 <span class="n">plt</span><span class="p">.</span><span class="nf">xlabel</span><span class="p">(</span><span class="sh">'</span><span class="s">kpois_sample</span><span class="sh">'</span><span class="p">)</span>
 <span class="n">plt</span><span class="p">.</span><span class="nf">title</span><span class="p">(</span><span class="sh">'</span><span class="s">distribution of Poisson random sample</span><span class="sh">'</span><span class="p">)</span>
 <span class="n">plt</span><span class="p">.</span><span class="nf">show</span><span class="p">()</span>
</pre></td> </tr></tbody></table></code></pre></figure> <p>You should get something that looks like this:</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/mle/pois_sample.png" sizes="95vw"></source> <img src="/assets/img/mle/pois_sample.png" class="img-fluid" width="400" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <h2 id="define-likelihood-function-1">Define likelihood function</h2> <p>For a <a href="https://en.wikipedia.org/wiki/Likelihood_function#Discrete_probability_distribution" rel="external nofollow noopener" target="_blank">discrete distribution</a> such as the Poisson, the likelihood function doesn’t change much, except it takes the products of the probability mass functions (PMF) rather than the probability distribution function (PDF):</p> \[\mathcal{L}(\theta\mid \mathbf{x})=p_{\theta}(\mathbf{x})=P_{\theta}(X=\mathbf{x})\] <p>or,</p> \[\mathcal{L}(\theta\mid \mathbf{x})=\prod_{i=1}^{N} p_{\theta}(x_i) = \prod_{i=1}^{N} P_{\theta}(X=x_i)\] <p>Again, we will need to define the <a href="https://en.wikipedia.org/wiki/Poisson_distribution#Probability_of_events_for_a_Poisson_distribution" rel="external nofollow noopener" target="_blank">PMF for the Poisson distribution</a> to define our likelihood function. Because the Poisson is parameterized by \(\lambda\), \(\theta=[\lambda]\) for our likelihood function.</p> \[p_{\lambda}(k)=p(k \mid \lambda)=\exp^{-\lambda}\frac{\lambda^k}{k!}\] <p>Again, we will minimize over the negative LLF. The LLF for the Poisson is expressed as:</p> \[\log (\mathcal{L}(\theta\mid \mathbf{k}))=\sum_{i=1}^{N} \log (p_{\theta}(k_i)) = \sum_{i=1}^{N} \log (P_{\theta}(X=k_i))\] <h2 id="optimize-1">Optimize</h2> <figure class="highlight"><pre><code class="language-python" data-lang="python"><table class="rouge-table"><tbody><tr>
<td class="gutter gl"><pre class="lineno">1
2
3
4
5
6
7
</pre></td> <td class="code"><pre> <span class="k">def</span> <span class="nf">ll_pois</span><span class="p">(</span><span class="n">params</span><span class="p">,</span> <span class="n">k</span><span class="p">):</span>
     <span class="sh">'''</span><span class="s">params is list of parameters to estimate: [lambda]
     k is list of Poisson distributed values described by estimated parameter</span><span class="sh">'''</span>
    
     <span class="n">lmbd</span> <span class="o">=</span> <span class="n">params</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
     <span class="n">loglik</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">log</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="nf">exp</span><span class="p">(</span><span class="o">-</span><span class="n">lmbd</span><span class="p">)</span><span class="o">*</span><span class="p">(</span><span class="n">lmbd</span><span class="o">**</span><span class="n">k</span><span class="p">)</span><span class="o">/</span><span class="nf">factorial</span><span class="p">(</span><span class="n">k</span><span class="p">))</span>
     <span class="k">return</span> <span class="o">-</span><span class="n">loglik</span><span class="p">.</span><span class="nf">sum</span><span class="p">()</span>
</pre></td> </tr></tbody></table></code></pre></figure> <p>Note: When you want to set bounds but you only have one parameter to estimate, you need to format it as demonstrated below, or you will run into an error.</p> <figure class="highlight"><pre><code class="language-python" data-lang="python"><table class="rouge-table"><tbody><tr>
<td class="gutter gl"><pre class="lineno">1
2
3
4
5
6
</pre></td> <td class="code"><pre> <span class="n">pois_res</span> <span class="o">=</span> <span class="n">optimize</span><span class="p">.</span><span class="nf">minimize</span><span class="p">(</span><span class="n">fun</span> <span class="o">=</span> <span class="n">ll_pois</span><span class="p">,</span>
               <span class="n">x0</span> <span class="o">=</span> <span class="p">[</span><span class="mf">1e-6</span><span class="p">],</span>
               <span class="n">args</span> <span class="o">=</span> <span class="p">(</span><span class="n">poisson_sample</span><span class="p">),</span>
             <span class="n">bounds</span> <span class="o">=</span> <span class="p">((</span><span class="mf">1e-6</span><span class="p">,</span><span class="bp">None</span><span class="p">),))</span>

 <span class="n">pois_res</span>
</pre></td> </tr></tbody></table></code></pre></figure> <p>The output should look something like this:</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>      fun: 937.3870827020155
 hess_inv: &lt;1x1 LbfgsInvHessProduct with dtype=float64&gt;
      jac: array([-0.0001819])
  message: b'CONVERGENCE: REL_REDUCTION_OF_F_&lt;=_FACTR*EPSMCH'
     nfev: 56
      nit: 17
   status: 0
  success: True
        x: array([0.51099991])
</code></pre></div></div> <p>We can print the coefficients:</p> <figure class="highlight"><pre><code class="language-python" data-lang="python"><table class="rouge-table"><tbody><tr>
<td class="gutter gl"><pre class="lineno">1
2
</pre></td> <td class="code"><pre> <span class="nf">print</span><span class="p">(</span><span class="sh">'</span><span class="s">lambda = %.1f</span><span class="sh">'</span> <span class="o">%</span> <span class="n">LAMBDA</span><span class="p">)</span>
 <span class="nf">print</span><span class="p">(</span><span class="sh">'</span><span class="s">lambda_est = %.4f</span><span class="sh">'</span> <span class="o">%</span> <span class="n">pois_res</span><span class="p">.</span><span class="n">x</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
</pre></td> </tr></tbody></table></code></pre></figure> <p>scipy has built-in pdf and logpdf functions for a number of statistical distributions, so in practice you could use those functions instead of implementing them yourself.</p> </div> </article> <br> <hr> <br> <ul class="list-disc pl-8"></ul> <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2> <p class="mb-2">Here are some more articles you might like to read next:</p> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2023/glm/">Generalized linear models</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2023/ols/">Ordinary least squares</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2023/gradient-descent/">Gradient descent</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2023/python_nn/">A very, very basic neural network in Python</a> </li> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2025 Jessica L. Zhou. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/assets/js/copy_code.js?12775fdf7f95e901d7119054556e495f" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>addBackToTop();</script> </body> </html>