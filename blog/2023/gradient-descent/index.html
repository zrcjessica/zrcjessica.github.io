<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Gradient descent | Jessica L. Zhou </title> <meta name="author" content="Jessica L. Zhou"> <meta name="description" content="A Python-coded implementation of gradient descent for a linear regression on simulated data"> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://zrcjessica.github.io/blog/2023/gradient-descent/"> <script src="/assets/js/theme.js?9a0c749ec5240d9cda97bc72359a72c0"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>initTheme();</script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> <span class="font-weight-bold">Jessica</span> L. Zhou </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about </a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/">blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/photos/">photos </a> </li> <li class="nav-item "> <a class="nav-link" href="/repositories/">repositories </a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">cv </a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">Gradient descent</h1> <p class="post-meta"> Created in February 21, 2023 </p> <p class="post-tags"> <a href="/blog/2023"> <i class="fa-solid fa-calendar fa-sm"></i> 2023 </a>   ·   <a href="/blog/tag/machine-learning"> <i class="fa-solid fa-hashtag fa-sm"></i> machine_learning</a>   <a href="/blog/tag/gradient-descent"> <i class="fa-solid fa-hashtag fa-sm"></i> gradient_descent</a>   <a href="/blog/tag/backpropagation"> <i class="fa-solid fa-hashtag fa-sm"></i> backpropagation</a>   <a href="/blog/tag/jupyter"> <i class="fa-solid fa-hashtag fa-sm"></i> jupyter</a>   <a href="/blog/tag/code"> <i class="fa-solid fa-hashtag fa-sm"></i> code</a>   ·   <a href="/blog/category/tutorials"> <i class="fa-solid fa-tag fa-sm"></i> tutorials</a>   <a href="/blog/category/code"> <i class="fa-solid fa-tag fa-sm"></i> code</a> </p> </header> <article class="post-content"> <div id="markdown-content"> <p>Gradient descent is an optimization algorithm used to find the optimal weights for a model. While it is popularly used to tune the weights in neural networks, in this tutorial we will be using it to try to recover the coefficients used to simulate a toy dataset. We will also demonstrate the differences between batch, stochastic, and mini-batch gradient descent and defining some of the relevant terms. Here is the link to the Jupyter notebook for this tutorial on my <a href="https://github.com/zrcjessica/ml_concepts/blob/main/gradient_descent.ipynb" rel="external nofollow noopener" target="_blank">GitHub</a>.</p> <h1 id="simulating-the-data">Simulating the data</h1> <p>We will begin by simulating the data for this example. Our simulated dataset will contain \(n=10000\) samples using a linear function defined as:</p> \[f(x_1, x_2, x_3) = ax_1 + bx_2 + cx_3 + \varepsilon\] <p>Here, \({x_1,x_2,x_3}\) represent an input vector of size 3 and \(\varepsilon\) is noise.</p> <p>To do this, we will first simulate the coefficients of the model, \(a,b,c\) by picking 3 random integers between 1 and 10.</p> <figure class="highlight"><pre><code class="language-python" data-lang="python"><table class="rouge-table"><tbody><tr>
<td class="gutter gl"><pre class="lineno">1
2
3
4
5
6
7
</pre></td> <td class="code"><pre> <span class="kn">import</span> <span class="n">numpy</span> <span class="k">as</span> <span class="n">np</span>
 <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">seed</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
 
 <span class="c1"># simulate coeffs 
</span> <span class="n">coeffs</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">randint</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">10</span><span class="p">,</span><span class="n">size</span> <span class="o">=</span> <span class="mi">3</span><span class="p">)</span>
 
 <span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">Coefficients: a = %d, b = %d, c = %d</span><span class="sh">"</span> <span class="o">%</span> <span class="p">(</span><span class="n">coeffs</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">coeffs</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">coeffs</span><span class="p">[</span><span class="mi">2</span><span class="p">]))</span>
</pre></td> </tr></tbody></table></code></pre></figure> <p>Next, we will generate the inputs and the noise for each sample by randomly sampling from the standard normal distribution, and then calculate the outputs based on the equation defined above.</p> <figure class="highlight"><pre><code class="language-python" data-lang="python"><table class="rouge-table"><tbody><tr>
<td class="gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
</pre></td> <td class="code"><pre> <span class="c1"># define number of data points
</span> <span class="n">n</span> <span class="o">=</span> <span class="mi">10000</span>

 <span class="c1"># define inputs in dataset
</span> <span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">randn</span><span class="p">(</span><span class="n">n</span><span class="p">,</span><span class="mi">3</span><span class="p">)</span>

 <span class="c1"># define noise
</span> <span class="n">noise</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">randn</span><span class="p">(</span><span class="n">n</span><span class="p">)</span>

 <span class="c1"># get outputs for dataset
</span> <span class="n">y</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">sum</span><span class="p">(</span><span class="n">X</span><span class="o">*</span><span class="n">coeffs</span><span class="p">,</span> <span class="n">axis</span> <span class="o">=</span> <span class="mi">1</span><span class="p">)</span> <span class="o">+</span> <span class="n">noise</span>
</pre></td> </tr></tbody></table></code></pre></figure> <p>Just to get a better idea of what the data looks like, let’s coerce the dataset we’ve simulated into a data frame format and take a look at it.</p> <figure class="highlight"><pre><code class="language-python" data-lang="python"><table class="rouge-table"><tbody><tr>
<td class="gutter gl"><pre class="lineno">1
2
3
</pre></td> <td class="code"><pre> <span class="kn">import</span> <span class="n">pandas</span> <span class="k">as</span> <span class="n">pd</span>

 <span class="n">pd</span><span class="p">.</span><span class="nc">DataFrame</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="nf">hstack</span><span class="p">((</span><span class="n">X</span><span class="p">,</span> <span class="n">np</span><span class="p">.</span><span class="nf">reshape</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">)))),</span> <span class="n">columns</span> <span class="o">=</span> <span class="p">[</span><span class="sh">"</span><span class="s">x1</span><span class="sh">"</span><span class="p">,</span><span class="sh">"</span><span class="s">x2</span><span class="sh">"</span><span class="p">,</span><span class="sh">"</span><span class="s">x3</span><span class="sh">"</span><span class="p">,</span><span class="sh">"</span><span class="s">y</span><span class="sh">"</span><span class="p">])</span>
</pre></td> </tr></tbody></table></code></pre></figure> <p>Now let’s cover the concepts that will be important for this tutorial.</p> <h1 id="gradient-descent">Gradient descent</h1> <p>Gradient descent is an optimization method that is popularly used in machine learning to find the best parameters (more often referred to as <em>weights</em> in the case of neural networks) for a given model. It works quite like how it sounds - by following gradients to descend towards the minimum of a <strong>cost function</strong>. Cost functions are a function of the difference between the true and predicted outputs of a given model. There are a number of different cost functions out there. For example, cross entropy cost functions are popularly used for classification problems while squared error cost functions are popular used for regression problems. Conceptually, gradient descent looks something like this:</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/gradient_descent/gradient-descent.jpeg" sizes="95vw"></source> <img src="/assets/img/gradient_descent/gradient-descent.jpeg" class="img-fluid" width="600" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <p><a href="https://saugatbhattarai.com.np/what-is-gradient-descent-in-machine-learning/" rel="external nofollow noopener" target="_blank">Image source</a></p> <p>The y-axis represents the possible values of the cost function, or error, evaluated for different model weights \(\mathbf{w} = \{w_1, w_2,...,w_j\}\) where \(j\) is the total number of weights in the model. In the beginning of the model training process, we have random weight values which will yield different errors. At each iteration of training, we will calculate the gradient at the point in the cost function given the current weights, multiply the gradient by a pre-determined <strong>learning rate</strong>, and then descend along the cost function curve accordingly.</p> <h1 id="types-of-gradient-descent">Types of gradient descent</h1> <p>There are a couple of different types of gradient descent out there. In order to understand the differences between these, it is first helpful to define some of the following terms:</p> <ul> <li> <strong>sample</strong>: a sample is a single data point that can be passed to the model</li> <li> <strong>batch</strong>: a hyperparameter which defines the number of samples that the model must evaluate before updating the weights in the model</li> <li> <strong>epoch</strong>: a hyperparameter that defines the number of time that the entire training dataset will be passed through the model</li> </ul> <p>Now, we can start talking about the different types of gradient descent!</p> <h2 id="batch-gradient-descent">Batch gradient descent</h2> <p>In batch gradient descent, every data point in our dataset is evaluated in a given training iteration and the gradients are summed for each data point and then used to make a single update to the weights in the model. In other words, <strong>the size of a batch is equivalent to the size of the entire training data set</strong>, and the number of batches in an epoch is 1.</p> <h5 id="pros">Pros</h5> <ul> <li>Because we only update the model after evaluating all data points, this results in fewer updates and a more computationally efficient training process</li> <li>Fewer udpates results in a more stable gradient and more stable convergence</li> </ul> <h5 id="cons">Cons</h5> <ul> <li>The more stable gradient may result in the model converging earlier to a less optimal set of parameters, e.g. a local minimum instead of a global minimum</li> <li>Prediction errors must be accumulated across all training samples because the model weights are updated after evaluation of all samples</li> <li>Usually the entire dataset needs to be loaded in memory for the model to work with it</li> <li>Altogether these cons make this approach slower</li> </ul> <h2 id="stochastic-gradient-descent-sgd">Stochastic gradient descent (SGD)</h2> <p>In stochastic gradient descent, a random subset of the training dataset is evaluated in each training iteration to provide a single update to the weights in the model. Typically, SGD refers to a random subset size of 1; that is, <strong>each batch consists of a single sample</strong>. The number of batches in a single epoch, then, is equivalent to the number of samples in the entire training data set.</p> <h5 id="pros-1">Pros</h5> <ul> <li>Because the model weights are updated more frequently, we can have a higher resolution of the how the model performs and how quickly it’s learning</li> <li>The higher frequency of model updates may help the model learn faster for some problems</li> <li>The “noisier” model updates may help the model avoid local minima</li> </ul> <h5 id="cons-1">Cons</h5> <ul> <li>Updating the model more frequently is more computational expensive</li> <li>The more frequent model updates result in noisier gradients, resulting in more variance in the error landscape across training epochs</li> <li>The noisier updates can also make it harder for the algorithm to optimize</li> </ul> <h2 id="stochastic-mini-batch-gradient-descent">(Stochastic) mini-batch gradient descent</h2> <p>As with SGD, we are picking random subsets of the data to pass through the model in order to inform the updating of the weights; however, here <strong>the batch size is somewhere between a single sample and the entirety of the training data set</strong>. Therefore, in a single epoch we see a number of samples roughly equivalent to the total number of samples in the training dataset divided by the batch size. This is very popular for training neural networks.</p> <h5 id="pros-2">Pros</h5> <ul> <li>More frequency model updates than batch gradient descent allows for a more robust convergence and a better likelihood of avoiding local minima</li> <li>Less frequent model updates than SGD results in greater computational efficiency</li> <li>Smaller batches means that we don’t have to have the entire training dataset in memory (as with batch gradient descent)</li> </ul> <h5 id="cons-2">Cons</h5> <ul> <li>We have to define an additional batch size hyperparameter</li> </ul> <h1 id="cost-function">Cost function</h1> <p>I will be using two closely related terms in this section: <strong>loss function</strong> and <strong>cost function</strong>. A <strong>loss function</strong> is calculated for a single data point while a <strong>cost function</strong> is the sum of the <strong>losses</strong> across all the points in a batch. For our tutorial, we will use the <strong>Least Squared Error</strong> loss function, which is commonly used for linear regression. For a given sample \(j\), it is defined as:</p> \[LSE = \frac{1}{2}(\hat{y}_j - y_j)^2\] <p>where \(\hat{y}_i\) is the predicted output for a given input vector \(\mathbf{x_i}\). Its associated cost function is known as <strong>Mean Squared Error (MSE)</strong>:</p> \[MSE = \frac{1}{2m} \sum _{j=1} ^{m} (\hat{y}_j - y_j)^2\] <p>where \(m\) is the total number of training samples.</p> <p>In our example, \(\hat{y}_j\) is calculated as a linear function that is essentially the dot product between the weights in the model, \(\mathbf{w}\) and the inputs for a given sample, \(\mathbf{x}_j\):</p> \[\hat{y}_j = w_1 x_{i_1} + w_2 x_{i_2} + w_3 x_{i_3}\] <p>Thus, the cost function can be expression as a function of \(\mathbf{w}\):</p> \[J(\mathbf{w}) = \frac{1}{2m} \sum _{j=1} ^{m} ((w_1 x_{j_1} + w_2 x_{j_2} + w_3 x_{j_3}) - y_j)^2\] <p>By training on the data we simulated using the coefficients \(a, b, c\), we are trying to optimize the weights \(w_1, w_2, w_3\) to try to recover the coefficients used to generate the data.</p> <p>Here, let’s define the functions we’ll use for calculating \(\hat{y}_j\) and MSE:</p> <figure class="highlight"><pre><code class="language-python" data-lang="python"><table class="rouge-table"><tbody><tr>
<td class="gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
</pre></td> <td class="code"><pre> <span class="k">def</span> <span class="nf">pred</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">weights</span><span class="p">):</span>
     <span class="sh">"""</span><span class="s">
     predict y given inputs and weights
     </span><span class="sh">"""</span>
     <span class="n">y_pred</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">dot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">weights</span><span class="p">)</span>
     <span class="k">return</span> <span class="n">y_pred</span>


 <span class="k">def</span> <span class="nf">mse</span><span class="p">(</span><span class="n">y_pred</span><span class="p">,</span> <span class="n">y_true</span><span class="p">,</span> <span class="n">m</span><span class="p">):</span>
     <span class="sh">"""</span><span class="s">
     calculate MSE 
     </span><span class="sh">"""</span>
     <span class="n">mse</span> <span class="o">=</span> <span class="p">(</span><span class="mi">1</span><span class="o">/</span><span class="p">(</span><span class="mi">2</span><span class="o">*</span><span class="n">m</span><span class="p">))</span><span class="o">*</span><span class="n">np</span><span class="p">.</span><span class="nf">sum</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="nf">square</span><span class="p">(</span><span class="n">y_pred</span> <span class="o">-</span> <span class="n">y_true</span><span class="p">))</span>
     <span class="k">return</span> <span class="n">mse</span>
</pre></td> </tr></tbody></table></code></pre></figure> <h1 id="learning-rate">Learning rate</h1> <p>The learning rate is a small value that determines how far along the curve we move to update the weights. It’s important to pick the right learning rate - a large learning rate can result in overshotting the optimum value, while a small learning rate will make it take much longer for the model to converge on the optimum.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/gradient_descent/learning_rate.jpeg" sizes="95vw"></source> <img src="/assets/img/gradient_descent/learning_rate.jpeg" class="img-fluid" width="600" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <p><a href="https://saugatbhattarai.com.np/what-is-gradient-descent-in-machine-learning/" rel="external nofollow noopener" target="_blank">Image source</a></p> <p>The learning rate will be denoted by \(\eta\). In our tutorial, let’s assign a learning rate of \(\eta = 0.01\).</p> <figure class="highlight"><pre><code class="language-python" data-lang="python"><table class="rouge-table"><tbody><tr>
<td class="gutter gl"><pre class="lineno">1
2
</pre></td> <td class="code"><pre><span class="c1"># learning rate
</span> <span class="n">lr</span> <span class="o">=</span> <span class="mf">0.01</span>
</pre></td> </tr></tbody></table></code></pre></figure> <h1 id="gradient-descent-algorithm">Gradient descent algorithm</h1> <p>In each iteration of the training algorithm, we will update each weight in our model \(w_i \rightarrow w_i'\) with the following formula:</p> \[w_i' = w_i - \eta \frac{\partial{J}}{\partial{w_i}}\] <p>where</p> \[\frac{\partial{J}}{\partial{w_i}} = \frac{1}{m} \sum _{j=1} ^{m} ((w_1 x_{j_1} + w_2 x_{j_2} + w_3 x_{j_3}) - y_i)(x_{j_i})\] <p>Let’s define the function for calculating the gradient of the cost function:</p> <figure class="highlight"><pre><code class="language-python" data-lang="python"><table class="rouge-table"><tbody><tr>
<td class="gutter gl"><pre class="lineno">1
2
3
4
5
6
</pre></td> <td class="code"><pre> <span class="k">def</span> <span class="nf">grad</span><span class="p">(</span><span class="n">m</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">,</span> <span class="n">y_true</span><span class="p">,</span> <span class="n">x_i</span><span class="p">):</span>
     <span class="sh">"""</span><span class="s">
     calculate the partial derivative of cost function wrt w_i
     </span><span class="sh">"""</span>
     <span class="n">grad</span> <span class="o">=</span> <span class="p">(</span><span class="mi">1</span><span class="o">/</span><span class="n">m</span><span class="p">)</span> <span class="o">*</span> <span class="n">np</span><span class="p">.</span><span class="nf">sum</span><span class="p">((</span><span class="n">y_pred</span> <span class="o">-</span> <span class="n">y_true</span><span class="p">)</span><span class="o">*</span><span class="n">x_i</span><span class="p">)</span>
     <span class="k">return</span> <span class="n">grad</span>
</pre></td> </tr></tbody></table></code></pre></figure> <p>Now let’s begin implementing our code! We’ll start with an example of <strong>batch gradient descent</strong>.</p> <h1 id="example---batch-gradient-descent">Example - batch gradient descent</h1> <p>In batch gradient descent, each epoch contains only one batch, and the batch size is equivalent to the entire size of the training dataset. We will define 1000 epochs.</p> <figure class="highlight"><pre><code class="language-python" data-lang="python"><table class="rouge-table"><tbody><tr>
<td class="gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
</pre></td> <td class="code"><pre> <span class="n">epochs</span> <span class="o">=</span> <span class="mi">1000</span>
 <span class="n">m</span> <span class="o">=</span> <span class="n">n</span>
 
 <span class="c1"># initialize random weights to start
</span> <span class="n">weights</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">randn</span><span class="p">(</span><span class="mi">3</span><span class="p">)</span>
 <span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">initial weights:</span><span class="sh">"</span><span class="p">)</span>
 <span class="nf">print</span><span class="p">(</span><span class="n">weights</span><span class="p">)</span>

 <span class="c1"># collect value of cost function at each iter
</span> <span class="n">cost_list</span> <span class="o">=</span> <span class="p">[]</span>

 <span class="c1"># collect the weights at the end of each epoch
</span> <span class="n">weights_updates</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">zeros</span><span class="p">((</span><span class="n">epochs</span><span class="o">+</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">))</span>
 <span class="n">weights_updates</span><span class="p">[</span><span class="mi">0</span><span class="p">,:]</span> <span class="o">=</span> <span class="n">weights</span>

 <span class="n">start</span> <span class="o">=</span> <span class="n">time</span><span class="p">.</span><span class="nf">time</span><span class="p">()</span>
 <span class="k">for</span> <span class="nb">iter</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">epochs</span><span class="p">):</span>
     <span class="nf">print</span><span class="p">(</span><span class="sh">'</span><span class="s">epoch = %d</span><span class="sh">'</span> <span class="o">%</span> <span class="nb">iter</span><span class="p">)</span>
          
     <span class="c1"># predict on training set 
</span>     <span class="n">y_pred</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">apply_along_axis</span><span class="p">(</span><span class="n">pred</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">arr</span> <span class="o">=</span> <span class="n">X</span><span class="p">,</span> <span class="n">weights</span> <span class="o">=</span> <span class="n">weights</span><span class="p">)</span>

     <span class="c1"># calculate cost
</span>     <span class="n">cost</span> <span class="o">=</span> <span class="nf">mse</span><span class="p">(</span><span class="n">y_pred</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">m</span><span class="p">)</span>
     <span class="n">cost_list</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">cost</span><span class="p">)</span>

     <span class="c1"># update the weights
</span>     <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">weights</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]):</span>
         <span class="n">weights</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">weights</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">-</span> <span class="n">lr</span><span class="o">*</span><span class="nf">grad</span><span class="p">(</span><span class="n">m</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">X</span><span class="p">[:,</span><span class="n">i</span><span class="p">])</span>

     <span class="nf">print</span><span class="p">(</span><span class="n">weights</span><span class="p">)</span>
     <span class="n">weights_updates</span><span class="p">[</span><span class="nb">iter</span><span class="o">+</span><span class="mi">1</span><span class="p">,</span> <span class="p">:]</span> <span class="o">=</span> <span class="n">weights</span>

 <span class="n">end</span> <span class="o">=</span> <span class="n">time</span><span class="p">.</span><span class="nf">time</span><span class="p">()</span>

 <span class="c1"># calculate time
</span> <span class="n">batch_gd_time</span> <span class="o">=</span> <span class="n">end</span> <span class="o">-</span> <span class="n">start</span>
</pre></td> </tr></tbody></table></code></pre></figure> <p>Let’s visualize the MSE over each training iteration:</p> <figure class="highlight"><pre><code class="language-python" data-lang="python"><table class="rouge-table"><tbody><tr>
<td class="gutter gl"><pre class="lineno">1
2
3
4
5
</pre></td> <td class="code"><pre> <span class="n">plt</span><span class="p">.</span><span class="nf">plot</span><span class="p">(</span><span class="n">cost_list</span><span class="p">)</span>
 <span class="n">plt</span><span class="p">.</span><span class="nf">ylabel</span><span class="p">(</span><span class="sh">"</span><span class="s">Cost</span><span class="sh">"</span><span class="p">)</span>
 <span class="n">plt</span><span class="p">.</span><span class="nf">xlabel</span><span class="p">(</span><span class="sh">"</span><span class="s">epoch</span><span class="sh">"</span><span class="p">)</span>
 <span class="n">plt</span><span class="p">.</span><span class="nf">title</span><span class="p">(</span><span class="sh">"</span><span class="s">MSE over training epochs - batch gradient descent</span><span class="sh">"</span><span class="p">)</span>
 <span class="n">plt</span><span class="p">.</span><span class="nf">show</span><span class="p">()</span>
</pre></td> </tr></tbody></table></code></pre></figure> <p>You should see something like this:</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/gradient_descent/mse_bgd.png" sizes="95vw"></source> <img src="/assets/img/gradient_descent/mse_bgd.png" class="img-fluid" width="400" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <p>Let’s also visualize how the weights changed at the end of each training epoch:</p> <figure class="highlight"><pre><code class="language-python" data-lang="python"><table class="rouge-table"><tbody><tr>
<td class="gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
</pre></td> <td class="code"><pre> <span class="n">plt</span><span class="p">.</span><span class="nf">plot</span><span class="p">(</span><span class="n">weights_updates</span><span class="p">[:,</span><span class="mi">0</span><span class="p">],</span> <span class="n">label</span> <span class="o">=</span> <span class="sh">"</span><span class="s">w1</span><span class="sh">"</span><span class="p">)</span>
 <span class="n">plt</span><span class="p">.</span><span class="nf">plot</span><span class="p">(</span><span class="n">weights_updates</span><span class="p">[:,</span><span class="mi">1</span><span class="p">],</span> <span class="n">label</span> <span class="o">=</span> <span class="sh">"</span><span class="s">w2</span><span class="sh">"</span><span class="p">)</span>
 <span class="n">plt</span><span class="p">.</span><span class="nf">plot</span><span class="p">(</span><span class="n">weights_updates</span><span class="p">[:,</span><span class="mi">2</span><span class="p">],</span> <span class="n">label</span> <span class="o">=</span> <span class="sh">"</span><span class="s">w3</span><span class="sh">"</span><span class="p">)</span>
 <span class="n">plt</span><span class="p">.</span><span class="nf">legend</span><span class="p">()</span>
 <span class="n">plt</span><span class="p">.</span><span class="nf">ylabel</span><span class="p">(</span><span class="sh">'</span><span class="s">weights</span><span class="sh">'</span><span class="p">)</span>
 <span class="n">plt</span><span class="p">.</span><span class="nf">xlabel</span><span class="p">(</span><span class="sh">"</span><span class="s">epochs</span><span class="sh">"</span><span class="p">)</span>
 <span class="n">plt</span><span class="p">.</span><span class="nf">title</span><span class="p">(</span><span class="sh">"</span><span class="s">Weights over training epochs - batch gradient descent</span><span class="sh">"</span><span class="p">)</span>
 <span class="n">plt</span><span class="p">.</span><span class="nf">show</span><span class="p">()</span>
</pre></td> </tr></tbody></table></code></pre></figure> <p>You should see something like this:</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/gradient_descent/weights_bgd.png" sizes="95vw"></source> <img src="/assets/img/gradient_descent/weights_bgd.png" class="img-fluid" width="400" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <p>Now let’s examine <code class="language-plaintext highlighter-rouge">weights</code>. It should be a numpy array that contains values that look something like this:</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>array([1.00021995, 1.00020721, 2.02081245])
</code></pre></div></div> <p>Out of curiosity, let’s run linear regression on our data and see how the coefficients obtained with the regression compare to what we got with our implementation of gradient descent.</p> <figure class="highlight"><pre><code class="language-python" data-lang="python"><table class="rouge-table"><tbody><tr>
<td class="gutter gl"><pre class="lineno">1
2
3
4
</pre></td> <td class="code"><pre> <span class="kn">from</span> <span class="n">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">LinearRegression</span>

 <span class="n">reg</span> <span class="o">=</span> <span class="nc">LinearRegression</span><span class="p">().</span><span class="nf">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
 <span class="n">reg</span><span class="p">.</span><span class="n">coef_</span>
</pre></td> </tr></tbody></table></code></pre></figure> <p>The resulting coefficients probably look something like this:</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>array([1.00032104, 1.00020243, 2.02069302])
</code></pre></div></div> <p>If we compare them to the coefficients we used to simulate this dataset, we can see that they are indeed very close to one another!</p> <figure class="highlight"><pre><code class="language-python" data-lang="python"><table class="rouge-table"><tbody><tr>
<td class="gutter gl"><pre class="lineno">1
2
</pre></td> <td class="code"><pre> <span class="n">model_results</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="nc">DataFrame</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">([</span><span class="n">weights</span><span class="p">,</span> <span class="n">reg</span><span class="p">.</span><span class="n">coef_</span><span class="p">,</span> <span class="n">coeffs</span><span class="p">]).</span><span class="nf">transpose</span><span class="p">(),</span> <span class="n">columns</span> <span class="o">=</span> <span class="p">[</span><span class="sh">'</span><span class="s">batch gradient descent</span><span class="sh">'</span><span class="p">,</span><span class="sh">'</span><span class="s">linear regression</span><span class="sh">'</span><span class="p">,</span><span class="sh">'</span><span class="s">true coeffs</span><span class="sh">'</span><span class="p">])</span>
 <span class="n">model_results</span>
</pre></td> </tr></tbody></table></code></pre></figure> <table> <thead> <tr> <th>index</th> <th>batch gradient descent</th> <th>linear regression</th> <th>true coeffs</th> </tr> </thead> <tbody> <tr> <td>0</td> <td>1.0002417538638582</td> <td>1.0003210378934184</td> <td>1.0</td> </tr> <tr> <td>1</td> <td>1.000179104020437</td> <td>1.000202425f449444</td> <td>1.0</td> </tr> <tr> <td>2</td> <td>2.0206704597237226</td> <td>2.020693020186785</td> <td>2.0</td> </tr> </tbody> </table> <h1 id="example---sgd">Example - SGD</h1> <p>Now we’ll implement SGD. We will adapt the code from earlier, except now each batch is of size 1 (\(m=1\)).</p> <figure class="highlight"><pre><code class="language-python" data-lang="python"><table class="rouge-table"><tbody><tr>
<td class="gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
</pre></td> <td class="code"><pre> <span class="c1"># define batch size
</span> <span class="n">m</span> <span class="o">=</span> <span class="mi">1</span>

 <span class="c1"># reinitialize weights
</span> <span class="n">weights</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">randn</span><span class="p">(</span><span class="mi">3</span><span class="p">)</span>

 <span class="c1"># collect avg cost at each epoch
</span> <span class="n">avg_cost_list</span> <span class="o">=</span> <span class="p">[]</span>

 <span class="c1"># collect the updated weights at each epoch
</span> <span class="n">weights_updates</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">zeros</span><span class="p">((</span><span class="n">epochs</span><span class="o">+</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">))</span>
 <span class="n">weights_updates</span><span class="p">[</span><span class="mi">0</span><span class="p">,:]</span> <span class="o">=</span> <span class="n">weights</span>

 <span class="n">start</span> <span class="o">=</span> <span class="n">time</span><span class="p">.</span><span class="nf">time</span><span class="p">()</span>
 <span class="k">for</span> <span class="nb">iter</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">epochs</span><span class="p">):</span>
     <span class="nf">print</span><span class="p">(</span><span class="sh">'</span><span class="s">epoch = %d</span><span class="sh">'</span> <span class="o">%</span> <span class="nb">iter</span><span class="p">)</span>
    
     <span class="c1"># collect losses for every batch in epoch
</span>     <span class="n">epoch_loss</span> <span class="o">=</span> <span class="p">[]</span>

     <span class="k">for</span> <span class="n">batch</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">X</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">//</span><span class="n">m</span><span class="p">):</span>
         <span class="c1"># predict on training set 
</span>         <span class="n">y_pred</span> <span class="o">=</span> <span class="nf">pred</span><span class="p">(</span><span class="n">X</span><span class="p">[</span><span class="n">batch</span><span class="p">,:],</span> <span class="n">weights</span><span class="p">)</span>

         <span class="c1"># calculate cost
</span>         <span class="n">cost</span> <span class="o">=</span> <span class="nf">mse</span><span class="p">(</span><span class="n">y_pred</span><span class="p">,</span> <span class="n">y</span><span class="p">[</span><span class="n">batch</span><span class="p">],</span> <span class="n">m</span><span class="p">)</span>
         <span class="n">epoch_loss</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">cost</span><span class="p">)</span>
        
         <span class="c1"># update the weights
</span>         <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">weights</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]):</span>
             <span class="n">weights</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">weights</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">-</span> <span class="n">lr</span><span class="o">*</span><span class="nf">grad</span><span class="p">(</span><span class="n">m</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">,</span> <span class="n">y</span><span class="p">[</span><span class="n">batch</span><span class="p">],</span> <span class="n">X</span><span class="p">[</span><span class="n">batch</span><span class="p">,</span><span class="n">i</span><span class="p">])</span>
       
     <span class="c1"># save avg cost for epoch across all batches
</span>     <span class="n">avg_cost_list</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="nf">average</span><span class="p">(</span><span class="n">epoch_loss</span><span class="p">))</span>
     <span class="n">weights_updates</span><span class="p">[</span><span class="nb">iter</span><span class="o">+</span><span class="mi">1</span><span class="p">,:]</span> <span class="o">=</span> <span class="n">weights</span>

 <span class="n">end</span> <span class="o">=</span> <span class="n">time</span><span class="p">.</span><span class="nf">time</span><span class="p">()</span>

 <span class="n">sgd_time</span> <span class="o">=</span> <span class="n">end</span> <span class="o">-</span> <span class="n">start</span>
</pre></td> </tr></tbody></table></code></pre></figure> <p>Now let’s plot the avefrage MSE across each training epoch:</p> <figure class="highlight"><pre><code class="language-python" data-lang="python"><table class="rouge-table"><tbody><tr>
<td class="gutter gl"><pre class="lineno">1
2
3
4
5
</pre></td> <td class="code"><pre> <span class="n">plt</span><span class="p">.</span><span class="nf">plot</span><span class="p">(</span><span class="n">avg_cost_list</span><span class="p">)</span>
 <span class="n">plt</span><span class="p">.</span><span class="nf">ylabel</span><span class="p">(</span><span class="sh">"</span><span class="s">Avg. cost</span><span class="sh">"</span><span class="p">)</span>
 <span class="n">plt</span><span class="p">.</span><span class="nf">xlabel</span><span class="p">(</span><span class="sh">"</span><span class="s">epoch</span><span class="sh">"</span><span class="p">)</span>
 <span class="n">plt</span><span class="p">.</span><span class="nf">title</span><span class="p">(</span><span class="sh">"</span><span class="s">Avg. MSE over training epochs - SGD</span><span class="sh">"</span><span class="p">)</span>
 <span class="n">plt</span><span class="p">.</span><span class="nf">show</span><span class="p">()</span>
</pre></td> </tr></tbody></table></code></pre></figure> <p>You should see something like this:</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/gradient_descent/avg_mse_sgd.png" sizes="95vw"></source> <img src="/assets/img/gradient_descent/avg_mse_sgd.png" class="img-fluid" width="400" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <p>We’ll also plot how the weights changed after each training epoch:</p> <figure class="highlight"><pre><code class="language-python" data-lang="python"><table class="rouge-table"><tbody><tr>
<td class="gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
</pre></td> <td class="code"><pre> <span class="n">plt</span><span class="p">.</span><span class="nf">plot</span><span class="p">(</span><span class="n">weights_updates</span><span class="p">[:,</span><span class="mi">0</span><span class="p">],</span> <span class="n">label</span> <span class="o">=</span> <span class="sh">"</span><span class="s">w1</span><span class="sh">"</span><span class="p">)</span>
 <span class="n">plt</span><span class="p">.</span><span class="nf">plot</span><span class="p">(</span><span class="n">weights_updates</span><span class="p">[:,</span><span class="mi">1</span><span class="p">],</span> <span class="n">label</span> <span class="o">=</span> <span class="sh">"</span><span class="s">w2</span><span class="sh">"</span><span class="p">)</span>
 <span class="n">plt</span><span class="p">.</span><span class="nf">plot</span><span class="p">(</span><span class="n">weights_updates</span><span class="p">[:,</span><span class="mi">2</span><span class="p">],</span> <span class="n">label</span> <span class="o">=</span> <span class="sh">"</span><span class="s">w3</span><span class="sh">"</span><span class="p">)</span>
 <span class="n">plt</span><span class="p">.</span><span class="nf">ylabel</span><span class="p">(</span><span class="sh">'</span><span class="s">weights</span><span class="sh">'</span><span class="p">)</span>
 <span class="n">plt</span><span class="p">.</span><span class="nf">xlabel</span><span class="p">(</span><span class="sh">"</span><span class="s">epoch</span><span class="sh">"</span><span class="p">)</span>
 <span class="n">plt</span><span class="p">.</span><span class="nf">legend</span><span class="p">()</span>
 <span class="n">plt</span><span class="p">.</span><span class="nf">title</span><span class="p">(</span><span class="sh">"</span><span class="s">Weights over training epochs - SGD</span><span class="sh">"</span><span class="p">)</span>
 <span class="n">plt</span><span class="p">.</span><span class="nf">show</span><span class="p">()</span>
</pre></td> </tr></tbody></table></code></pre></figure> <p>You should see something like this:</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/gradient_descent/weights_sgd.png" sizes="95vw"></source> <img src="/assets/img/gradient_descent/weights_sgd.png" class="img-fluid" width="400" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <p>In the previous example of batch gradient descent, it looked like our model began to converge around 200 training epochs. Here, because we updated the model weights after evaluating each sample, we appear to have actually minimized the cost very early on in the training process. However, the weights that the model has converged on seem to be slightly less accurate compared to batch gradient descent. Let’s add these results to the data frame and take a look:</p> <figure class="highlight"><pre><code class="language-python" data-lang="python"><table class="rouge-table"><tbody><tr>
<td class="gutter gl"><pre class="lineno">1
2
</pre></td> <td class="code"><pre> <span class="n">model_results</span><span class="p">[</span><span class="sh">'</span><span class="s">SGD</span><span class="sh">'</span><span class="p">]</span> <span class="o">=</span> <span class="n">weights</span>
 <span class="n">model_results</span>
</pre></td> </tr></tbody></table></code></pre></figure> <table> <thead> <tr> <th>index</th> <th>batch gradient descent</th> <th>linear regression</th> <th>true coeffs</th> <th>SGD</th> <th>mini-batch</th> </tr> </thead> <tbody> <tr> <td>0</td> <td>1.0002417538638582</td> <td>1.0003210378934184</td> <td>1.0</td> <td>1.1791146471366072</td> <td>0.9970121830081912</td> </tr> <tr> <td>1</td> <td>1.000179104020437</td> <td>1.000202425449444</td> <td>1.0</td> <td>1.0082643415913106</td> <td>1.0008372545371162</td> </tr> <tr> <td>2</td> <td>2.0206704597237226</td> <td>2.020693020186785</td> <td>2.0</td> <td>1.9850734474472451</td> <td>2.0208606244363096</td> </tr> </tbody> </table> <h1 id="example---mini-batch-stochastic-gradient-descent">Example - mini-batch stochastic gradient descent</h1> <p>Now let’s modify the code from the SGD example to accomodate mini-batch gradient descent. Let’s define the batch size as \(m=100\) to yield 100 batches per epoch. First we’ll define a function for partioning our data into batches:</p> <figure class="highlight"><pre><code class="language-python" data-lang="python"><table class="rouge-table"><tbody><tr>
<td class="gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
</pre></td> <td class="code"><pre> <span class="k">def</span> <span class="nf">batch</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">):</span>
     <span class="sh">'''</span><span class="s">
     code for partitioning dataset into batcehs
     </span><span class="sh">'''</span>
    
     <span class="c1"># shuffle indices of samples 
</span>     <span class="n">sample_ix</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">arange</span><span class="p">(</span><span class="n">n</span><span class="p">)</span>
     <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">shuffle</span><span class="p">(</span><span class="n">sample_ix</span><span class="p">)</span>
    
     <span class="n">batches</span> <span class="o">=</span> <span class="p">[]</span>

     <span class="k">for</span> <span class="n">batch_ix</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">n</span><span class="o">//</span><span class="n">m</span><span class="p">):</span>
         <span class="c1"># determine which samples to pick for batch
</span>         <span class="n">samples_in_batch</span> <span class="o">=</span> <span class="n">sample_ix</span><span class="p">[</span><span class="n">batch_ix</span><span class="o">*</span><span class="n">m</span><span class="p">:(</span><span class="n">batch_ix</span><span class="o">*</span><span class="n">m</span> <span class="o">+</span> <span class="n">m</span> <span class="p">)]</span>
         <span class="n">batches</span><span class="p">.</span><span class="nf">append</span><span class="p">([</span><span class="n">X</span><span class="p">[</span><span class="n">samples_in_batch</span><span class="p">,:],</span> <span class="n">y</span><span class="p">[</span><span class="n">samples_in_batch</span><span class="p">]])</span>

     <span class="k">return</span> <span class="n">batches</span>
</pre></td> </tr></tbody></table></code></pre></figure> <p>Now we’ll modify the code from our SGD example to implement mini-batch gradient descent:</p> <figure class="highlight"><pre><code class="language-python" data-lang="python"><table class="rouge-table"><tbody><tr>
<td class="gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
40
41
42
43
44
45
46
47
</pre></td> <td class="code"><pre> <span class="c1"># define batch size
</span> <span class="n">m</span> <span class="o">=</span> <span class="mi">100</span>

 <span class="c1"># reinitialize weights
</span> <span class="n">weights</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">randn</span><span class="p">(</span><span class="mi">3</span><span class="p">)</span>

 <span class="c1"># collect avg cost at each epoch
</span> <span class="n">avg_cost_list</span> <span class="o">=</span> <span class="p">[]</span>

 <span class="c1"># collect the updated weights at each epoch
</span> <span class="n">weights_updates</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">zeros</span><span class="p">((</span><span class="n">epochs</span><span class="o">+</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">))</span>
 <span class="n">weights_updates</span><span class="p">[</span><span class="mi">0</span><span class="p">,:]</span> <span class="o">=</span> <span class="n">weights</span>

 <span class="c1"># get batched data
</span> <span class="n">batches</span> <span class="o">=</span> <span class="nf">batch</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">m</span><span class="p">)</span>

 <span class="n">start</span> <span class="o">=</span> <span class="n">time</span><span class="p">.</span><span class="nf">time</span><span class="p">()</span>

 <span class="k">for</span> <span class="nb">iter</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">epochs</span><span class="p">):</span>
     <span class="nf">print</span><span class="p">(</span><span class="sh">'</span><span class="s">epoch = %d</span><span class="sh">'</span> <span class="o">%</span> <span class="nb">iter</span><span class="p">)</span>
    
     <span class="c1"># collect losses for every batch in epoch
</span>     <span class="n">epoch_loss</span> <span class="o">=</span> <span class="p">[]</span>

     <span class="k">for</span> <span class="n">batch_ix</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">X</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">//</span><span class="n">m</span><span class="p">):</span>
    
         <span class="n">batch_X</span> <span class="o">=</span> <span class="n">batches</span><span class="p">[</span><span class="n">batch_ix</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span>
         <span class="n">batch_y</span> <span class="o">=</span> <span class="n">batches</span><span class="p">[</span><span class="n">batch_ix</span><span class="p">][</span><span class="mi">1</span><span class="p">]</span>

         <span class="c1"># predict on training set 
</span>         <span class="n">y_pred</span> <span class="o">=</span> <span class="nf">pred</span><span class="p">(</span><span class="n">batch_X</span><span class="p">,</span> <span class="n">weights</span><span class="p">)</span>

         <span class="c1"># calculate cost
</span>         <span class="n">cost</span> <span class="o">=</span> <span class="nf">mse</span><span class="p">(</span><span class="n">y_pred</span><span class="p">,</span> <span class="n">batch_y</span><span class="p">,</span> <span class="n">m</span><span class="p">)</span>
         <span class="n">epoch_loss</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">cost</span><span class="p">)</span>
        
         <span class="c1"># update the weights
</span>         <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">weights</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]):</span>
             <span class="n">weights</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">weights</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">-</span> <span class="n">lr</span><span class="o">*</span><span class="nf">grad</span><span class="p">(</span><span class="n">m</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">,</span> <span class="n">batch_y</span><span class="p">,</span> <span class="n">batch_X</span><span class="p">[:,</span><span class="n">i</span><span class="p">])</span>
    
     <span class="c1"># save avg cost for epoch across all batches
</span>     <span class="n">avg_cost_list</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="nf">average</span><span class="p">(</span><span class="n">epoch_loss</span><span class="p">))</span>
     <span class="n">weights_updates</span><span class="p">[</span><span class="nb">iter</span><span class="o">+</span><span class="mi">1</span><span class="p">,:]</span> <span class="o">=</span> <span class="n">weights</span>

 <span class="n">end</span> <span class="o">=</span> <span class="n">time</span><span class="p">.</span><span class="nf">time</span><span class="p">()</span>

 <span class="n">minibatch_time</span> <span class="o">=</span> <span class="n">end</span> <span class="o">-</span> <span class="n">start</span>
</pre></td> </tr></tbody></table></code></pre></figure> <p>Now we’ll plot the average MSE at each epoch:</p> <figure class="highlight"><pre><code class="language-python" data-lang="python"><table class="rouge-table"><tbody><tr>
<td class="gutter gl"><pre class="lineno">1
2
3
4
5
</pre></td> <td class="code"><pre> <span class="n">plt</span><span class="p">.</span><span class="nf">plot</span><span class="p">(</span><span class="n">avg_cost_list</span><span class="p">)</span>
 <span class="n">plt</span><span class="p">.</span><span class="nf">ylabel</span><span class="p">(</span><span class="sh">"</span><span class="s">Avg. cost</span><span class="sh">"</span><span class="p">)</span>
 <span class="n">plt</span><span class="p">.</span><span class="nf">xlabel</span><span class="p">(</span><span class="sh">"</span><span class="s">epoch</span><span class="sh">"</span><span class="p">)</span>
 <span class="n">plt</span><span class="p">.</span><span class="nf">title</span><span class="p">(</span><span class="sh">"</span><span class="s">Avg. MSE over training epochs - mini-batch gradient descent</span><span class="sh">"</span><span class="p">)</span>
 <span class="n">plt</span><span class="p">.</span><span class="nf">show</span><span class="p">()</span>
</pre></td> </tr></tbody></table></code></pre></figure> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/gradient_descent/mini_batch_mse.png" sizes="95vw"></source> <img src="/assets/img/gradient_descent/mini_batch_mse.png" class="img-fluid" width="400" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <p>And how the weights changed after each epoch:</p> <figure class="highlight"><pre><code class="language-python" data-lang="python"><table class="rouge-table"><tbody><tr>
<td class="gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
</pre></td> <td class="code"><pre> <span class="n">plt</span><span class="p">.</span><span class="nf">plot</span><span class="p">(</span><span class="n">weights_updates</span><span class="p">[:,</span><span class="mi">0</span><span class="p">],</span> <span class="n">label</span> <span class="o">=</span> <span class="sh">"</span><span class="s">w1</span><span class="sh">"</span><span class="p">)</span>
 <span class="n">plt</span><span class="p">.</span><span class="nf">plot</span><span class="p">(</span><span class="n">weights_updates</span><span class="p">[:,</span><span class="mi">1</span><span class="p">],</span> <span class="n">label</span> <span class="o">=</span> <span class="sh">"</span><span class="s">w2</span><span class="sh">"</span><span class="p">)</span>
 <span class="n">plt</span><span class="p">.</span><span class="nf">plot</span><span class="p">(</span><span class="n">weights_updates</span><span class="p">[:,</span><span class="mi">2</span><span class="p">],</span> <span class="n">label</span> <span class="o">=</span> <span class="sh">"</span><span class="s">w3</span><span class="sh">"</span><span class="p">)</span>
 <span class="n">plt</span><span class="p">.</span><span class="nf">ylabel</span><span class="p">(</span><span class="sh">'</span><span class="s">weights</span><span class="sh">'</span><span class="p">)</span>
 <span class="n">plt</span><span class="p">.</span><span class="nf">xlabel</span><span class="p">(</span><span class="sh">"</span><span class="s">epoch</span><span class="sh">"</span><span class="p">)</span>
 <span class="n">plt</span><span class="p">.</span><span class="nf">legend</span><span class="p">()</span>
 <span class="n">plt</span><span class="p">.</span><span class="nf">title</span><span class="p">(</span><span class="sh">"</span><span class="s">Weights over training epochs - mini-batch gradient descent</span><span class="sh">"</span><span class="p">)</span>
 <span class="n">plt</span><span class="p">.</span><span class="nf">show</span><span class="p">()</span>
</pre></td> </tr></tbody></table></code></pre></figure> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/gradient_descent/mini_batch_weights.png" sizes="95vw"></source> <img src="/assets/img/gradient_descent/mini_batch_weights.png" class="img-fluid" width="400" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <p>Finally, we’ll compare the results we got from batch gradient descent to the other approaches:</p> <figure class="highlight"><pre><code class="language-python" data-lang="python"><table class="rouge-table"><tbody><tr>
<td class="gutter gl"><pre class="lineno">1
2
</pre></td> <td class="code"><pre> <span class="n">model_results</span><span class="p">[</span><span class="sh">'</span><span class="s">mini-batch</span><span class="sh">'</span><span class="p">]</span> <span class="o">=</span> <span class="n">weights</span>
 <span class="n">model_results</span>
</pre></td> </tr></tbody></table></code></pre></figure> <table> <thead> <tr> <th>index</th> <th>batch gradient descent</th> <th>linear regression</th> <th>true coeffs</th> <th>SGD</th> <th>mini-batch</th> </tr> </thead> <tbody> <tr> <td>0</td> <td>1.0002417538638582</td> <td>1.0003210378934184</td> <td>1.0</td> <td>1.1791146471366072</td> <td>0.9970121830081912</td> </tr> <tr> <td>1</td> <td>1.000179104020437</td> <td>1.000202425449444</td> <td>1.0</td> <td>1.0082643415913106</td> <td>1.0008372545371162</td> </tr> <tr> <td>2</td> <td>2.0206704597237226</td> <td>2.020693020186785</td> <td>2.0</td> <td>1.9850734474472451</td> <td>2.0208606244363096</td> </tr> </tbody> </table> <p>Let’s also compare the time it took to run these three different types of gradient descent:</p> <figure class="highlight"><pre><code class="language-python" data-lang="python"><table class="rouge-table"><tbody><tr>
<td class="gutter gl"><pre class="lineno">1
2
3
</pre></td> <td class="code"><pre> <span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">batch gradient descent time: %.3f</span><span class="sh">"</span> <span class="o">%</span> <span class="n">batch_gd_time</span><span class="p">)</span>
 <span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">SGD time: %.3f</span><span class="sh">"</span> <span class="o">%</span> <span class="n">sgd_time</span><span class="p">)</span>
 <span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">mini batch gradient descent time: %.3f</span><span class="sh">"</span> <span class="o">%</span> <span class="n">minibatch_time</span><span class="p">)</span>
</pre></td> </tr></tbody></table></code></pre></figure> <p>This will give you something that looks like this:</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>batch gradient descent time: 35.231
SGD time: 517.481
mini batch gradient descent time: 6.185
</code></pre></div></div> <p>Overall, all methods managed to converge on weights that are close to the true values we used to generate the data, and also comparable to the results of using sklearn’s linear regression function. However, we observed that SGD and mini-batch appeared to converge faster. Using SGD and mini-batch gradient descent, we could have probably reduced the number of epochs to reach the optimal weights in less time. SGD took very long to run because we updated the model weights after evaluating every single sample, and every single sample was evaluated 1000 times. Mini-batch gradient descent was much faster, and demonstrates that it is a good mix of the pros and cons of batch gradient descent and SGD.</p> </div> </article> <br> <hr> <br> <ul class="list-disc pl-8"></ul> <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2> <p class="mb-2">Here are some more articles you might like to read next:</p> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2023/glm/">Generalized linear models</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2023/ols/">Ordinary least squares</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2023/mle/">Maximum likelihood estimation</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2023/python_nn/">A very, very basic neural network in Python</a> </li> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2025 Jessica L. Zhou. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/assets/js/copy_code.js?12775fdf7f95e901d7119054556e495f" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>addBackToTop();</script> </body> </html>