<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> A very, very basic neural network in Python | Jessica L. Zhou </title> <meta name="author" content="Jessica L. Zhou"> <meta name="description" content="A Python-coded implementation of a two layer neural network using only numpy."> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://zrcjessica.github.io/blog/2023/python_nn/"> <script src="/assets/js/theme.js?9a0c749ec5240d9cda97bc72359a72c0"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>initTheme();</script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> <span class="font-weight-bold">Jessica</span> L. Zhou </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about </a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/">blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/photos/">photos </a> </li> <li class="nav-item "> <a class="nav-link" href="/repositories/">repositories </a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">cv </a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">A very, very basic neural network in Python</h1> <p class="post-meta"> Created in February 20, 2023 </p> <p class="post-tags"> <a href="/blog/2023"> <i class="fa-solid fa-calendar fa-sm"></i> 2023 </a>   ·   <a href="/blog/tag/machine-learning"> <i class="fa-solid fa-hashtag fa-sm"></i> machine_learning</a>   <a href="/blog/tag/deep-learning"> <i class="fa-solid fa-hashtag fa-sm"></i> deep_learning</a>   <a href="/blog/tag/neural-network"> <i class="fa-solid fa-hashtag fa-sm"></i> neural_network</a>   <a href="/blog/tag/gradient-descent"> <i class="fa-solid fa-hashtag fa-sm"></i> gradient_descent</a>   <a href="/blog/tag/backpropagation"> <i class="fa-solid fa-hashtag fa-sm"></i> backpropagation</a>   <a href="/blog/tag/jupyter"> <i class="fa-solid fa-hashtag fa-sm"></i> jupyter</a>   <a href="/blog/tag/code"> <i class="fa-solid fa-hashtag fa-sm"></i> code</a>   ·   <a href="/blog/category/tutorials"> <i class="fa-solid fa-tag fa-sm"></i> tutorials</a>   <a href="/blog/category/code"> <i class="fa-solid fa-tag fa-sm"></i> code</a> </p> </header> <article class="post-content"> <div id="markdown-content"> <p>When I first started learning about generalized linear models and optimization, my PI advised me to try implementing my own examples from scratch. I found this approach very helpful and have since started using this approach to try to gain a better understand of how a number of different aspects of machine learning work. We have so many powerful libraries for machine learning these days that sometimes you can get by with just a surface level understanding of how things work. There’s nothing wrong with that, but I sometimes get hung up when I don’t understand the details of how something works. Here, I’ve adapted an exercise from an excellent <a href="http://iamtrask.github.io/2015/07/12/basic-python-network/" rel="external nofollow noopener" target="_blank">post by Andrew Trask</a> on a toy example of a neural network implemented from scratch with Python.</p> <h1 id="task">Task</h1> <p>In this example, we are training a neural network to make a binary prediction (\(\{0,1\}\)) based on a binary input vector of size \(3\). The training data provided is:</p> <table class="tg"> <tr> <th class="tg-5rcs" colspan="3">Inputs</th> <th class="tg-5rcs">Output</th> </tr> <tr> <td class="tg-4kyz">0</td> <td class="tg-4kyz">0</td> <td class="tg-4kyz">1</td> <td class="tg-4kyz">0</td> </tr> <tr> <td class="tg-4kyz">1</td> <td class="tg-4kyz">1</td> <td class="tg-4kyz">1</td> <td class="tg-4kyz">1</td> </tr> <tr> <td class="tg-4kyz">1</td> <td class="tg-4kyz">0</td> <td class="tg-4kyz">1</td> <td class="tg-4kyz">1</td> </tr> <tr> <td class="tg-4kyz">0</td> <td class="tg-4kyz">1</td> <td class="tg-4kyz">1</td> <td class="tg-4kyz">0</td> </tr> </table> <p>From looking at this table, one might notice a pattern - the output is 1 whenever the first value in the input vector is also 1, suggesting a correlation between these two values. By training a neural network, it should also be able to learn this pattern.</p> <h1 id="the-neural-network-architecture">The neural network architecture</h1> <p>Our simple neural network will sipmly pass the inputs through a single neuron with a sigmoid activation function to return an output between 0 and 11. The neural network architecture can be visualized as follows:</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/python_nn/nn_arch.png" sizes="95vw"></source> <img src="/assets/img/python_nn/nn_arch.png" class="img-fluid" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <p>The first layer is the input channel, where each of the values in the input vector are fed into the neural network, and the second layer produces the output based on a transformation of the inputs.</p> <h1 id="training-the-neural-network">Training the neural network</h1> <p>To train the neural network, we will use every data point (row) in the data table and pass it through the network forwards and backwards \(n\) times. Over each pass, or iteration, we will update the weights in the neural network with the goal of improving the outputs at each iteration.</p> <h2 id="forward-propagation">Forward propagation</h2> <p>During forward propagation, each input row of our table (corresponds to one sample data point) will be passed through the first layer and multiplied by the hidden weights in the network via a dot product. There is a different weight for each input, producing a weighted sum:</p> \[s = \Sigma w_i x_i = w_1 x_1 + w_2 x_2 + w_3 x_3\] <p>This weighted sum is then passed to a sigmoid activation function to output a prediction, \(\hat{y} = \sigma(s)\). We will repeat this for \(n\) trianing iterations.</p> <h3 id="initializing-the-weights">Initializing the weights</h3> <p>The weights are the parameters of our neural network which we will be tuning over the course of training to try to get the most accurate predictions possible. We will start by initializing these with very small values (for simplicity’s sake, we will generate random values with a mean of 0).</p> <h3 id="activation-function">Activation function</h3> <p>As aforementioned we will be using a sigmoid activation function, which is defined as:</p> \[\sigma(x) = \frac{1}{1+e^{-x}}\] <p>Sigmoid functions are popular in machine learning because they yield an output between 0 and 1 and are also easily differentiable as:</p> \[\sigma ^\prime (x) = x(1-x)\] <p>The derivative is important for the backpropgation step of this neural network, which is the step that actually helps us update the weights in each iteration of the training process.</p> <h2 id="back-propagation">Back propagation</h2> <p>This is the part where we work backwards from the predicted output of the neural network to update the weights.</p> <h3 id="calculating-the-error">Calculating the error</h3> <p>After each forward pass through the network, we will get a prediction for each sample in the training data from the sigmoid function. From this, we can measure the error of the prediction given the current weights as the different between the prediction and the true output value and then use this information to update the weights. In machine learning, there are various different <strong>cost functions</strong> for measuring the difference between true and predicted values. While the original blog post simply refers to the error as the difference between the true and predicted outputs, I will use a squared error <strong>cost function</strong> and demonstrate further along this post why these approaches are equivalent. The squared error cost function I will use is expressed as:</p> \[E = \frac{1}{2}(y-\hat{y})^2\] <p>Here, \(y\) represents the true output while \(\hat{y}\) is the predicted output for a given training iteraction. In a more sophisticated neural network with multiple output channels, the error would be summed across all outputs, but our neural network only has one output so the expression is simpler. We add \(\frac{1}{2}\) to the beginning of this function because this function will be differentiated and the fraction allows the exponent to be canceled when differentiated. Because the derivative is typically multiplied by a learning rate anyways, it doesn’t matter that a constant is introduced here.</p> <h3 id="updating-the-weights---gradient-descent">Updating the weights - gradient descent</h3> <p>The magnitude of the error from the cost function will let us know how we should adjust the current weights in the network to achieve more accurate predictions. That is, we want to adjust the weights, \(\mathbf{w}\), in response to the error, \(E\). Essentially, we’re asking the question: <strong>how does the error change as the weights change?</strong> The mathematical of this expression is \(\frac{ \partial{E} }{ \partial{w_i} }\), or the partial derivative of \(E\) with respect to a given weight \(w_i\). We can calculate this using a method called <strong>gradient descent</strong>. Gradient descent follows the curve of a function to find the set of inputs that yields the minimum output of the function. In our case, we are following the curve of the cost function to find the weights that yield the minimum error! Gradient is another way of referring to the derivative of a function - it follows that gradient descent is executed using derivatives.</p> <h4 id="example">Example</h4> <p>To get a clearer idea of how gradient descent works, we will walk through the process of updating a single weight, \(w_1\), in our model. In order to do this, we need to find all of the intermediate functions that relate \(E\) to \(w_1\). It might be easier if we visualize this:</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/python_nn/backprop_diagram.png" sizes="95vw"></source> <img src="/assets/img/python_nn/backprop_diagram.png" class="img-fluid" width="500" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <p>We know that the cost function is calculated from \(y\) and \(\hat{y}\), that \(hat{y}\) is calculated as \(\sigma(s)\), and that \(s\) is simply the weighted sum of the inputs and the weights \(w_1 x_1 + w_2 x_2 + w_3 x_3\). Now we have all of the functions that relate \(E\) and \(w_1\)! The reason why we want all of the intermediate functions is because we will be using the chain rule (from calculus) to calculate \(\frac{\partial{E}}{\partial{w_1}}\):</p> \[\frac{\partial{E}}{\partial{w_1}} = \frac{\partial{E}}{\partial{\hat{y}}} \frac{\partial{\hat{y}}}{\partial{s}} \frac{\partial{s}}{\partial{w_1}}\] <p>The chain rule essentially breaks things down into a product of the partial derivatives of all of the intermediate functions relating \(E\) to \(w_1\).</p> <p>The partial derivative of \(E\) with respect to \(\hat{y}\) is:</p> \[\frac{\partial{E}}{\partial{\hat{y}}} = 2*\frac{1}{2}(y-\hat{y})^{2-1}*-1 = \hat{y}-y\] <p>The partial derivative of \(\hat{y}\) with respect to \(s\) is:</p> \[\frac{\partial{\hat{y}}}{\partial{s}} = \sigma ^\prime (\hat{y}) = \hat{y}(1-\hat{y})\] <p>Finally, the partial derivative of \(s\) with respect to \(w_1\) is:</p> \[\frac{\partial{s}}{\partial{w_1}}=x_1+0+0 = x_1\] <p>Therefore, for a given training iteration \(j\), we will generate an updated value of \(w_{1_{j+1}}\) by subtracting \(\frac{\partial{E}}{\partial{w_1}}\) from the value of \(w_1\) in the current iteration (\(w_{1_j}\)). This is because moving in the <strong>opposite</strong> direction of the gradient of the error will help us reach a minimum error. By extension, the value of \(\frac{\partial{E}}{\partial{w_i}}\) is the quantity by which we update any given weight \(w_i\). Typically in gradient descent, the gradient will also be multiplied by a carefully chosen step size, or learning rate, \(\eta\), but for the purpose of our example we will use \(\eta=1\). This will end up working out fine for our example, but it may not be the best learning rate for more complicated tasks and datasets.</p> <h4 id="revisiting-the-original-example">Revisiting the original example</h4> <p>In Andrew Trask’s original post, he updates the weights according to the formula: \(input * error * output(1-output)\). We can see that this expression is equivalent to the expression we have derived using gradient descent. The \(input\) in his expresison is equivalent to \(x_i\) in ours; he defined \(error\) as the difference between the true and predicted outputs, or \(\hat{y}-y\) in ours; and \(output\) in his expression is simply \(\hat{y}\) in ours. I was originally confused by where his formula for updating the weights came from, so I wanted to sit down and really prove where it came from to make sure I understood. Hope this helps someone else out there too!</p> <h3 id="summing-over-a-batch">Summing over a batch</h3> <p>During each iteration of our training loop, we will evaluate all four samples (this is known as <strong>batch gradient descent</strong>). How do we integrate data from the four samples in one batch to make a single update to each value of \(w\)? For a given weight \(w_i\), an input value at the corresponding position \(x_i\) will yield a different value of \(\frac{\partial{E}}{\partial{w_i}}\) for updating the weights. We will simply sum the values obtained across all samples for each \(w_{i_j}\) to obtain a single update quantity \(w_{i_{j+1}}\). We will implement this in the code with vectorization.</p> <h1 id="implementation">Implementation</h1> <p>Here is the code! We will work on implementing this simple neural network with the provided training data using only numpy (and matplotlib for visualization). This tutorial can also be found as a <a href="https://github.com/zrcjessica/ml_concepts/blob/main/python_nn.ipynb" rel="external nofollow noopener" target="_blank">Jupyter notebook on GitHub</a>.</p> <figure class="highlight"><pre><code class="language-python" data-lang="python"><table class="rouge-table"><tbody><tr>
<td class="gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
40
41
42
43
44
45
46
47
48
49
50
51
52
53
54
55
56
57
58
59
60
61
62
63
64
65
66
67
68
69
70
71
72
73
74
75
76
77
78
79
80
81
82
83
84
85
86
87
88
89
90
91
92
93
94
95
</pre></td> <td class="code"><pre> <span class="kn">import</span> <span class="n">numpy</span> <span class="k">as</span> <span class="n">np</span>
 <span class="kn">import</span> <span class="n">matplotlib.pyplot</span> <span class="k">as</span> <span class="n">plt</span>

 <span class="c1"># random seed for reproducibility
</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">seed</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>

 <span class="c1"># a class for the simple two-layer neural network
</span> <span class="k">class</span> <span class="nc">SimpleNN</span><span class="p">():</span>
    <span class="sh">"""</span><span class="s">
    initialize a simple feed-forward two layer neural network:
    - input layer of variable size
    - output layer of fixed size (1)
    </span><span class="sh">"""</span>
    
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">input_size</span> <span class="o">=</span> <span class="mi">3</span><span class="p">):</span>
        <span class="sh">"""</span><span class="s">
        initialize weights in network and declare input size
        </span><span class="sh">"""</span>
        <span class="n">self</span><span class="p">.</span><span class="n">input_size</span> <span class="o">=</span> <span class="n">input_size</span>
        <span class="c1"># random weights with mean = 0
</span>        <span class="n">self</span><span class="p">.</span><span class="n">weights</span> <span class="o">=</span> <span class="mi">2</span><span class="o">*</span><span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">random</span><span class="p">((</span><span class="n">input_size</span><span class="p">,</span><span class="mi">1</span><span class="p">))</span><span class="o">-</span><span class="mi">1</span>
    
    <span class="k">def</span> <span class="nf">__sigmoid</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">weighted_sum</span><span class="p">):</span>
        <span class="sh">"""</span><span class="s">
        sigmoid function intended to take as input weight sum of weights and inputs
        </span><span class="sh">"""</span>
        <span class="k">return</span> <span class="mi">1</span><span class="o">/</span><span class="p">(</span><span class="mi">1</span><span class="o">+</span><span class="n">np</span><span class="p">.</span><span class="nf">exp</span><span class="p">(</span><span class="o">-</span><span class="n">weighted_sum</span><span class="p">))</span>
        
    <span class="k">def</span> <span class="nf">__gradient</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="sh">"""</span><span class="s">
        computes derivative of sigmoid function for a given value of x
        </span><span class="sh">"""</span>
        <span class="k">return</span> <span class="n">x</span><span class="o">*</span><span class="p">(</span><span class="mi">1</span><span class="o">-</span><span class="n">x</span><span class="p">)</span>
        
    <span class="k">def</span> <span class="nf">__accuracy</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">pred</span><span class="p">,</span> <span class="n">target</span><span class="p">):</span>
        <span class="sh">"""</span><span class="s">
        given true and predicted outputs, calculate accuracy
        </span><span class="sh">"""</span>
        <span class="c1"># binarize predicted values
</span>        <span class="n">pred_bin</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">where</span><span class="p">(</span><span class="n">pred</span><span class="o">&gt;</span><span class="mf">0.5</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>
        <span class="n">accuracy</span> <span class="o">=</span> <span class="p">(</span><span class="n">pred_bin</span> <span class="o">==</span> <span class="n">target</span><span class="p">).</span><span class="nf">sum</span><span class="p">()</span><span class="o">/</span><span class="n">target</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        <span class="k">return</span> <span class="n">accuracy</span>
        
    <span class="k">def</span> <span class="nf">train</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">trainX</span><span class="p">,</span> <span class="n">trainY</span><span class="p">,</span> <span class="n">iters</span> <span class="o">=</span> <span class="mi">1000</span><span class="p">,</span> <span class="n">verbose</span> <span class="o">=</span> <span class="bp">False</span><span class="p">):</span>
        <span class="sh">"""</span><span class="s">
        function for training neural network on training data
        </span><span class="sh">"""</span>
        <span class="n">accuracy</span> <span class="o">=</span> <span class="p">[]</span>
        
        <span class="k">for</span> <span class="nb">iter</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">iters</span><span class="p">):</span>
            <span class="c1">### feed forward ###
</span>            
            <span class="c1"># calculate weighted sum
</span>            <span class="n">s</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">dot</span><span class="p">(</span><span class="n">trainX</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">weights</span><span class="p">)</span>
            
            <span class="c1"># predict yhat
</span>            <span class="n">yhat</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">__sigmoid</span><span class="p">(</span><span class="n">s</span><span class="p">)</span>
            
            <span class="c1">### backprop ###
</span>            
            <span class="c1"># calculate cost function
</span>            <span class="n">err</span> <span class="o">=</span> <span class="n">trainY</span> <span class="o">-</span> <span class="n">yhat</span>
            
            <span class="c1"># gradient descent
</span>            <span class="n">grad</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">dot</span><span class="p">(</span><span class="n">trainX</span><span class="p">.</span><span class="n">T</span><span class="p">,</span> <span class="p">(</span><span class="n">yhat</span> <span class="o">-</span> <span class="n">trainY</span><span class="p">)</span><span class="o">*</span><span class="n">self</span><span class="p">.</span><span class="nf">__gradient</span><span class="p">(</span><span class="n">yhat</span><span class="p">))</span>
            
            <span class="c1"># update weights
</span>            <span class="n">self</span><span class="p">.</span><span class="n">weights</span> <span class="o">-=</span> <span class="n">grad</span> 
            
            <span class="c1"># calculate accuracy
</span>            <span class="n">accu_iter</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">__accuracy</span><span class="p">(</span><span class="n">yhat</span><span class="p">,</span> <span class="n">trainY</span><span class="p">)</span>
            <span class="n">accuracy</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">accu_iter</span><span class="p">)</span>
            
            <span class="k">if</span> <span class="n">verbose</span><span class="p">:</span>
                <span class="k">if</span> <span class="nb">iter</span><span class="o">%</span><span class="mi">100</span> <span class="o">==</span><span class="mi">0</span> <span class="ow">or</span> <span class="nb">iter</span><span class="o">==</span><span class="mi">0</span><span class="p">:</span>
                    <span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">iter %d</span><span class="sh">"</span> <span class="o">%</span> <span class="nb">iter</span><span class="p">)</span>
                    <span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">output</span><span class="sh">"</span><span class="p">)</span>
                    <span class="nf">print</span><span class="p">(</span><span class="n">yhat</span><span class="p">)</span>
                    <span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">accuracy = %.2f</span><span class="se">\n</span><span class="sh">"</span> <span class="o">%</span> <span class="n">accu_iter</span><span class="p">)</span>
            
        <span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">output after training:</span><span class="sh">"</span><span class="p">)</span>
        <span class="nf">print</span><span class="p">(</span><span class="n">yhat</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">accuracy</span> <span class="o">=</span> <span class="n">accuracy</span>
            
    <span class="k">def</span> <span class="nf">pred</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">testX</span><span class="p">):</span>
        <span class="sh">"""</span><span class="s">
        given a previously unseen set of input data, predict output
        </span><span class="sh">"""</span>
        <span class="c1"># calculate weighted sum of inputs and weights
</span>        <span class="n">s</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">dot</span><span class="p">(</span><span class="n">testX</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">weights</span><span class="p">)</span>
        
        <span class="c1"># predict yhat
</span>        <span class="n">yhat</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">__sigmoid</span><span class="p">(</span><span class="n">s</span><span class="p">)</span>
        
        <span class="k">return</span> <span class="n">yhat</span>
</pre></td> </tr></tbody></table></code></pre></figure> <p>We’ll start by initializing our neural network and checking out the starting weights:</p> <figure class="highlight"><pre><code class="language-python" data-lang="python"><table class="rouge-table"><tbody><tr>
<td class="gutter gl"><pre class="lineno">1
2
3
4
</pre></td> <td class="code"><pre> <span class="n">nn</span> <span class="o">=</span> <span class="nc">SimpleNN</span><span class="p">(</span><span class="mi">3</span><span class="p">)</span>
 
 <span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">starting weights</span><span class="sh">"</span><span class="p">)</span>
 <span class="nf">print</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">weights</span><span class="p">)</span>
</pre></td> </tr></tbody></table></code></pre></figure> <p>The output will look something like this:</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>starting weights
[[-0.16595599]
 [ 0.44064899]
 [-0.99977125]]
</code></pre></div></div> <p>Now let’s load in the training data as numpy arrays:</p> <figure class="highlight"><pre><code class="language-python" data-lang="python"><table class="rouge-table"><tbody><tr>
<td class="gutter gl"><pre class="lineno">1
2
3
4
5
6
</pre></td> <td class="code"><pre> <span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">([[</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">],</span>
              <span class="p">[</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">],</span>
              <span class="p">[</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">],</span>
              <span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">]])</span>
              
 <span class="n">y</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">([[</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">]]).</span><span class="n">T</span>
</pre></td> </tr></tbody></table></code></pre></figure> <p>Now we’ll train the network over 1,000 training iterations!</p> <figure class="highlight"><pre><code class="language-python" data-lang="python"><table class="rouge-table"><tbody><tr>
<td class="gutter gl"><pre class="lineno">1
</pre></td> <td class="code"><pre> <span class="n">nn</span><span class="p">.</span><span class="nf">train</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
</pre></td> </tr></tbody></table></code></pre></figure> <p>You’ll get an output that looks something like this:</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>output after training:
[[0.03178421]
 [0.97414645]
 [0.97906682]
 [0.02576499]]
</code></pre></div></div> <p>What are the weights after training and how have they changed from the initial weights? We can check with the function <code class="language-plaintext highlighter-rouge">nn.weights</code>, which will give you something like this:</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>array([[ 7.26283009],
       [-0.21614618],
       [-3.41703015]])
</code></pre></div></div> <p>We can see that \(w_1\) is by far larger than \(w_2\) and \(w_3\), which is in line with the relationship we observed between \(x_1\) and the output \(y\) at the beginning of this tutorial.</p> <p>We can also try visualizing the accuracy over each training iteration:</p> <figure class="highlight"><pre><code class="language-python" data-lang="python"><table class="rouge-table"><tbody><tr>
<td class="gutter gl"><pre class="lineno">1
2
3
4
</pre></td> <td class="code"><pre> <span class="n">plt</span><span class="p">.</span><span class="nf">plot</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">accuracy</span><span class="p">)</span>
 <span class="n">plt</span><span class="p">.</span><span class="nf">xlabel</span><span class="p">(</span><span class="sh">'</span><span class="s">training iter</span><span class="sh">'</span><span class="p">)</span>
 <span class="n">plt</span><span class="p">.</span><span class="nf">ylabel</span><span class="p">(</span><span class="sh">'</span><span class="s">accuracy</span><span class="sh">'</span><span class="p">)</span>
 <span class="n">plt</span><span class="p">.</span><span class="nf">show</span><span class="p">()</span>
</pre></td> </tr></tbody></table></code></pre></figure> <p>You should get something that looks like this:</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/python_nn/acc_plot.png" sizes="95vw"></source> <img src="/assets/img/python_nn/acc_plot.png" class="img-fluid" width="500" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <p>The accuracy improves pretty quickly here, likely because we have such a simple example.</p> <h1 id="prediction">Prediction</h1> <p>Just to make it interesting, let’s try predicting on a new data point where the input vector is \({1,0,0}\) and the corresponding true output is \(y=1\).</p> <figure class="highlight"><pre><code class="language-python" data-lang="python"><table class="rouge-table"><tbody><tr>
<td class="gutter gl"><pre class="lineno">1
2
3
4
</pre></td> <td class="code"><pre> <span class="n">testX</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">])</span>
 <span class="n">testY</span> <span class="o">=</span> <span class="mi">1</span>

 <span class="n">nn</span><span class="p">.</span><span class="nf">pred</span><span class="p">(</span><span class="n">testX</span><span class="p">)</span>
</pre></td> </tr></tbody></table></code></pre></figure> <p>If the predicted output is very close to 1, the binarizing this probability will yield a match very close to the true target value of this data point. The neural network has been trained the identify the pattern in the data and make accurate predictions!</p> </div> </article> <br> <hr> <br> <ul class="list-disc pl-8"></ul> <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2> <p class="mb-2">Here are some more articles you might like to read next:</p> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2023/glm/">Generalized linear models</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2023/ols/">Ordinary least squares</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2023/mle/">Maximum likelihood estimation</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2023/gradient-descent/">Gradient descent</a> </li> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2025 Jessica L. Zhou. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/assets/js/copy_code.js?12775fdf7f95e901d7119054556e495f" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>addBackToTop();</script> </body> </html>