<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Generalized linear models | Jessica L. Zhou </title> <meta name="author" content="Jessica L. Zhou"> <meta name="description" content="A Python-coded implementation of binomial generalized linear model"> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://zrcjessica.github.io/blog/2023/glm/"> <script src="/assets/js/theme.js?9a0c749ec5240d9cda97bc72359a72c0"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>initTheme();</script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> <span class="font-weight-bold">Jessica</span> L. Zhou </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about </a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/">blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/photos/">photos </a> </li> <li class="nav-item "> <a class="nav-link" href="/repositories/">repositories </a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">cv </a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">Generalized linear models</h1> <p class="post-meta"> Created in February 28, 2023 </p> <p class="post-tags"> <a href="/blog/2023"> <i class="fa-solid fa-calendar fa-sm"></i> 2023 </a>   ·   <a href="/blog/tag/machine-learning"> <i class="fa-solid fa-hashtag fa-sm"></i> machine_learning</a>   <a href="/blog/tag/glm"> <i class="fa-solid fa-hashtag fa-sm"></i> glm</a>   <a href="/blog/tag/modeling"> <i class="fa-solid fa-hashtag fa-sm"></i> modeling</a>   <a href="/blog/tag/statistics"> <i class="fa-solid fa-hashtag fa-sm"></i> statistics</a>   ·   <a href="/blog/category/tutorials"> <i class="fa-solid fa-tag fa-sm"></i> tutorials</a>   <a href="/blog/category/code"> <i class="fa-solid fa-tag fa-sm"></i> code</a> </p> </header> <article class="post-content"> <div id="markdown-content"> <p>Generalized linear models (GLMs) are something I’ve worked a lot with during my PhD. This tutorial is adapted from a lesson I developed for the bootcamp course I taught for first years in my PhD program. A Jupyter notebook for this tutorial can be found on my <a href="https://github.com/zrcjessica/ml_concepts/blob/main/GLM_tutorial.ipynb" rel="external nofollow noopener" target="_blank">GitHub</a>. As the name implies, GLMs are a generalized form of linear regression. Linear regression models make a number of assumptions about the data:</p> <ul> <li>there must be a linear relationship between the response variable and the predictors (<strong>linearity</strong>)</li> <li>the predictor variables, \(x\), can be treated as fixed values rather than random variables (<strong>exogeneity</strong>)</li> <li>the variance of the errors does not depend on the values of the predictorr variables, \(x\) (<strong>homoscedacity</strong>)</li> <li>the errors of the response variables, \(y\), are not correlated with one another (<strong>independence of errors</strong>)</li> <li>there does not exist a linear relationship between two or more of the predictor variables (<strong>lack of perfect multicollinearity</strong>)</li> </ul> <h1 id="ordinary-linear-regression-and-its-assumptions">Ordinary linear regression and its assumptions</h1> <p>Ordinary linear regression predicts the expected value of a response variable, \(y\), as a linear combination of predictors, \(\mathbf{x}\). That is, for a given data point \(i\) in a data set of size \(n\), \(\{ y_i,x_{i1}, ..., x_{ip} \} _{i=1}^{n}\), a linear regression model assumes that:</p> \[y_i = \beta_0 + \beta_1 x_{i1} + ... + \beta_p x_{ip} + \varepsilon _i = \mathbf{x}_i ^T \mathbf{\beta} \forall i=1,...,n\] <p>where \(\varepsilon\) is an error term that adds noise to the linear relationship between the predictors and the response variables. This expression can also be expressed in matrix notation as</p> \[\mathbf{y} = \mathbf{X \beta} + \mathbf{\varepsilon}\] <p><a href="https://en.wikipedia.org/wiki/Linear_regression" rel="external nofollow noopener" target="_blank">Source</a></p> <p>One of the implications of ordinary linear regression is that a constant change in the predictors leads to a constant change in the response variables (a <strong>linear-response model</strong>). However, this assumption is violated by certain types of repsonse variables. For example, when the response variable is restricted to being a positive value, or when the response variable is non-linear with relation to the predictors, or when the response is binary or categorical. Generalized linear models solve this problem by allowing for response variables that are not restricted to being normally distributed. Instead, it allows for some function of the response variable, known as the <strong>link function</strong>, to have a linear relationship with the predictors.</p> <h1 id="glms-and-link-functions">GLMs and link functions</h1> <p>In a GLM, it is assumed that each response variable \(y\) comes from some statistical distribution in the <a href="https://en.wikipedia.org/wiki/Exponential_family" rel="external nofollow noopener" target="_blank">exponential family</a>. These inclued, but are not limited to, the normal distribution. The mean, or expected value, of the response variable \(y\) given the predictors \(\mathbf{X}\) can be expressed as:</p> \[E(\mathbf{y}|\mathbf{X}) = \mathbf{\mu} = g^{-1}(\mathbf{X}\beta)\] <p>Here, \(g\) is a link function that relates the <strong>linear predictor</strong> \(\mathbf{X \beta}\) to the expected value of the response variable \(y\) conditional upon the predictors \(\mathbf{X}\). The link function used depends on the distribution of the response variables you’re working with.</p> <h1 id="load-data-for-tutorial">Load data for tutorial</h1> <p>In this tutorial, we will work with a dataset containing information about California standardized testing results (STAR test) for grades 2-11. This dataset is loaded with the <a href="https://www.statsmodels.org/dev/datasets/generated/star98.html" rel="external nofollow noopener" target="_blank">statsmodels package</a> and contains test results for 303 unified school districts. Here, the binary response variables respresent the number of 9th graders scoring above the national median for the math section. Notes about the data can be found at the link, or viewed with the command <code class="language-plaintext highlighter-rouge">print(sm.datasets.star98.NOTE)</code>. Let’s break down the data:</p> <ul> <li> <code class="language-plaintext highlighter-rouge">NABOVE</code> and <code class="language-plaintext highlighter-rouge">NBELOW</code> are the binary response variable</li> <li> <code class="language-plaintext highlighter-rouge">LOWINC</code> through <code class="language-plaintext highlighter-rouge">PCTYRRND</code> are the <strong>12</strong> independent variables, or regressors. These would be represented by \(\mathbf X_1,...,\mathbf X_p\) for \(p\) regressors.</li> <li>There are <strong>8</strong> interaction terms, representing non-linear interactions between two or more regressors. When an interaction is present, the effect of a regressor on the response variable depends on the value(s) of the variable(s) which it interacts with. The values of these interaction variables is simply the product of its interacting terms; for example, if variables \(A\) and \(B\) are interacting, the value of its interaction term, \(\mathbf X_{AB} = \mathbf X_A \circ \mathbf X_B\).</li> </ul> <p>Now, let’s load the data as a data frame:</p> <figure class="highlight"><pre><code class="language-python" data-lang="python"><table class="rouge-table"><tbody><tr>
<td class="gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
</pre></td> <td class="code"><pre> 
 <span class="c1"># import packages for tutorial 
</span> <span class="kn">import</span> <span class="n">statsmodels.api</span> <span class="k">as</span> <span class="n">sm</span>
 <span class="kn">import</span> <span class="n">pandas</span> <span class="k">as</span> <span class="n">pd</span>
 <span class="kn">import</span> <span class="n">numpy</span> <span class="k">as</span> <span class="n">np</span>
 <span class="kn">from</span> <span class="n">scipy</span> <span class="kn">import</span> <span class="n">stats</span> 
 <span class="kn">import</span> <span class="n">matplotlib.pyplot</span> <span class="k">as</span> <span class="n">plt</span>
 <span class="kn">import</span> <span class="n">seaborn</span> <span class="k">as</span> <span class="n">sns</span>
 <span class="kn">import</span> <span class="n">math</span>
 <span class="kn">from</span> <span class="n">scipy</span> <span class="kn">import</span> <span class="n">optimize</span>
 
 <span class="c1"># load dataset as pandas dataframe
</span> <span class="n">data</span> <span class="o">=</span> <span class="n">sm</span><span class="p">.</span><span class="n">datasets</span><span class="p">.</span><span class="n">star98</span><span class="p">.</span><span class="nf">load_pandas</span><span class="p">()</span>
</pre></td> </tr></tbody></table></code></pre></figure> <p>In statsmodels, <code class="language-plaintext highlighter-rouge">endog</code> refers to the response variable(s). In this case, it will return the values of <code class="language-plaintext highlighter-rouge">NABOVE</code> and <code class="language-plaintext highlighter-rouge">NBELOW</code>. Running <code class="language-plaintext highlighter-rouge">data.endog.head()</code> should return a data frame that looks like this:</p> <table> <thead> <tr> <th>index</th> <th>NABOVE</th> <th>NBELOW</th> </tr> </thead> <tbody> <tr> <td>0</td> <td>452.0</td> <td>355.0</td> </tr> <tr> <td>1</td> <td>144.0</td> <td>40.0</td> </tr> <tr> <td>2</td> <td>337.0</td> <td>234.0</td> </tr> <tr> <td>3</td> <td>395.0</td> <td>178.0</td> </tr> <tr> <td>4</td> <td>8.0</td> <td>57.0</td> </tr> </tbody> </table> <p><code class="language-plaintext highlighter-rouge">exog</code> refers to all of the other independent variables/regressors/interaction terms. Running <code class="language-plaintext highlighter-rouge">data.exog.head()</code> should return a data frame that contains the rest of the dsata.</p> <h2 id="visualize-the-data">Visualize the data</h2> <p>Let’s visualize the data and get a high-level idea of what’s going on. To start, let’s use seaborn to visualize the relationship between each of the independent variables (excluding the interaction terms) and the percentage of 9th grade students in each district scoring above the national median on the math section of the STAR test (100*<code class="language-plaintext highlighter-rouge">NABOVE</code>/(<code class="language-plaintext highlighter-rouge">NABOVE</code>+<code class="language-plaintext highlighter-rouge">NBELOW</code>)).</p> <p>It will be convenient to first reformat the data table into a form that is more friendly for exploratory data visualization. If you recall, each column represents a variable. We will reshape the table such that for each of the response variables (<code class="language-plaintext highlighter-rouge">id_vars</code>), there is one entry for each of the regressors (<code class="language-plaintext highlighter-rouge">value_vars</code>). By default, any columns not set as <code class="language-plaintext highlighter-rouge">id_vars</code> will be interpreted as <code class="language-plaintext highlighter-rouge">value_vars</code>. Each sample will have a record of the variable name and value. This process is called <a href="https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.melt.html" rel="external nofollow noopener" target="_blank"><strong>melting</strong></a> the dataframe.</p> <p>Here, we will subset the data table to the columns containing the binary response variables (<code class="language-plaintext highlighter-rouge">NABOVE</code> and <code class="language-plaintext highlighter-rouge">NBELOW</code>) and the first 12 variables (excluding the interaction terms). This corresponds to the first 14 columns of the dataframe.</p> <figure class="highlight"><pre><code class="language-python" data-lang="python"><table class="rouge-table"><tbody><tr>
<td class="gutter gl"><pre class="lineno">1
2
</pre></td> <td class="code"><pre> <span class="n">plot_df</span> <span class="o">=</span> <span class="n">data</span><span class="p">.</span><span class="n">data</span><span class="p">.</span><span class="n">iloc</span><span class="p">[:,:</span><span class="mi">14</span><span class="p">].</span><span class="nf">melt</span><span class="p">(</span><span class="n">id_vars</span> <span class="o">=</span> <span class="p">[</span><span class="sh">'</span><span class="s">NABOVE</span><span class="sh">'</span><span class="p">,</span><span class="sh">'</span><span class="s">NBELOW</span><span class="sh">'</span><span class="p">])</span>
 <span class="n">plot_df</span><span class="p">.</span><span class="nf">head</span><span class="p">()</span> 
</pre></td> </tr></tbody></table></code></pre></figure> <p>This should return a data frame that looks something like this: |index|NABOVE|NBELOW|variable|value| |—|—|—|—|—| |0|452.0|355.0|LOWINC|34.3973| |1|144.0|40.0|LOWINC|17.36507| |2|337.0|234.0|LOWINC|32.64324| |3|395.0|178.0|LOWINC|11.90953| |4|8.0|57.0|LOWINC|36.88889|</p> <p>Next, we add columns corresponding to the percentage of students above the national median (<code class="language-plaintext highlighter-rouge">PCTABOVE</code>) and the percentage of students below the national median (<code class="language-plaintext highlighter-rouge">PCTBELOW</code>).</p> <p>Then we will use <code class="language-plaintext highlighter-rouge">seaborn</code> to generate <a href="https://seaborn.pydata.org/generated/seaborn.regplot.html" rel="external nofollow noopener" target="_blank">line plots</a> relating <code class="language-plaintext highlighter-rouge">PCTABOVE</code> to each of the 12 independent variables selected.</p> <figure class="highlight"><pre><code class="language-python" data-lang="python"><table class="rouge-table"><tbody><tr>
<td class="gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
</pre></td> <td class="code"><pre> <span class="c1"># calculate PCTABOVE and PCTBELOW
</span> <span class="n">plot_df</span><span class="p">[</span><span class="sh">'</span><span class="s">PCTABOVE</span><span class="sh">'</span><span class="p">]</span> <span class="o">=</span> <span class="mi">100</span><span class="o">*</span><span class="n">plot_df</span><span class="p">[</span><span class="sh">'</span><span class="s">NABOVE</span><span class="sh">'</span><span class="p">]</span><span class="o">/</span><span class="p">(</span><span class="n">plot_df</span><span class="p">[</span><span class="sh">'</span><span class="s">NABOVE</span><span class="sh">'</span><span class="p">]</span><span class="o">+</span><span class="n">plot_df</span><span class="p">[</span><span class="sh">'</span><span class="s">NBELOW</span><span class="sh">'</span><span class="p">])</span>
 <span class="n">plot_df</span><span class="p">[</span><span class="sh">'</span><span class="s">PCTBELOW</span><span class="sh">'</span><span class="p">]</span> <span class="o">=</span> <span class="mi">100</span><span class="o">*</span><span class="n">plot_df</span><span class="p">[</span><span class="sh">'</span><span class="s">NBELOW</span><span class="sh">'</span><span class="p">]</span><span class="o">/</span><span class="p">(</span><span class="n">plot_df</span><span class="p">[</span><span class="sh">'</span><span class="s">NABOVE</span><span class="sh">'</span><span class="p">]</span><span class="o">+</span><span class="n">plot_df</span><span class="p">[</span><span class="sh">'</span><span class="s">NBELOW</span><span class="sh">'</span><span class="p">])</span>

 <span class="c1"># plot
</span> <span class="n">sns</span><span class="p">.</span><span class="nf">relplot</span><span class="p">(</span><span class="n">x</span> <span class="o">=</span> <span class="sh">'</span><span class="s">value</span><span class="sh">'</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="sh">'</span><span class="s">PCTABOVE</span><span class="sh">'</span><span class="p">,</span> <span class="n">hue</span> <span class="o">=</span> <span class="sh">'</span><span class="s">variable</span><span class="sh">'</span><span class="p">,</span> <span class="n">kind</span><span class="o">=</span><span class="sh">'</span><span class="s">line</span><span class="sh">'</span><span class="p">,</span>
              <span class="n">col</span> <span class="o">=</span> <span class="sh">'</span><span class="s">variable</span><span class="sh">'</span><span class="p">,</span> <span class="n">col_wrap</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">aspect</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">data</span> <span class="o">=</span> <span class="n">plot_df</span><span class="p">)</span>
 <span class="n">plt</span><span class="p">.</span><span class="nf">show</span><span class="p">()</span>
</pre></td> </tr></tbody></table></code></pre></figure> <p>This should give you a plot that looks like this:</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/glm/lineplots_regressors_vs_pctabove.png" sizes="95vw"></source> <img src="/assets/img/glm/lineplots_regressors_vs_pctabove.png" class="img-fluid" width="700" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <p>We can do the same thing but with <code class="language-plaintext highlighter-rouge">PCTBELOW</code> on the y-axis.</p> <figure class="highlight"><pre><code class="language-python" data-lang="python"><table class="rouge-table"><tbody><tr>
<td class="gutter gl"><pre class="lineno">1
2
3
4
</pre></td> <td class="code"><pre><span class="c1"># plot
</span> <span class="n">sns</span><span class="p">.</span><span class="nf">relplot</span><span class="p">(</span><span class="n">x</span> <span class="o">=</span> <span class="sh">'</span><span class="s">value</span><span class="sh">'</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="sh">'</span><span class="s">PCTBELOW</span><span class="sh">'</span><span class="p">,</span> <span class="n">hue</span> <span class="o">=</span> <span class="sh">'</span><span class="s">variable</span><span class="sh">'</span><span class="p">,</span> <span class="n">kind</span><span class="o">=</span><span class="sh">'</span><span class="s">line</span><span class="sh">'</span><span class="p">,</span>
              <span class="n">col</span> <span class="o">=</span> <span class="sh">'</span><span class="s">variable</span><span class="sh">'</span><span class="p">,</span> <span class="n">col_wrap</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">aspect</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">data</span> <span class="o">=</span> <span class="n">plot_df</span><span class="p">)</span>
 <span class="n">plt</span><span class="p">.</span><span class="nf">show</span><span class="p">()</span>
</pre></td> </tr></tbody></table></code></pre></figure> <p>This will give you a plot that looks like this:</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/glm/lineplots_regressors_vs_pctbelow.png" sizes="95vw"></source> <img src="/assets/img/glm/lineplots_regressors_vs_pctbelow.png" class="img-fluid" width="700" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <p>We see that the trends appear to be flipped, which is what we’d expect - variables that are positively correlated with the number of students scoring above the mean are most likely to be negatively correlated with the opposite outcome (number of students scoring below the mean).</p> <p>We can also take a look at hte distributions of the values of the independent variables (excluding interaction terms).</p> <figure class="highlight"><pre><code class="language-python" data-lang="python"><table class="rouge-table"><tbody><tr>
<td class="gutter gl"><pre class="lineno">1
2
3
4
</pre></td> <td class="code"><pre> <span class="n">plt</span><span class="p">.</span><span class="nf">figure</span><span class="p">(</span><span class="n">figsize</span> <span class="o">=</span> <span class="p">(</span><span class="mi">15</span><span class="p">,</span><span class="mi">10</span><span class="p">))</span>
 <span class="n">sns</span><span class="p">.</span><span class="nf">boxplot</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="sh">'</span><span class="s">variable</span><span class="sh">'</span><span class="p">,</span><span class="n">y</span><span class="o">=</span><span class="sh">'</span><span class="s">value</span><span class="sh">'</span><span class="p">,</span><span class="n">data</span> <span class="o">=</span> <span class="n">data</span><span class="p">.</span><span class="n">exog</span><span class="p">.</span><span class="n">iloc</span><span class="p">[:,:</span><span class="mi">12</span><span class="p">].</span><span class="nf">melt</span><span class="p">())</span>
 <span class="n">plt</span><span class="p">.</span><span class="nf">title</span><span class="p">(</span><span class="sh">'</span><span class="s">Distribution of explanatory variables (without interactions)</span><span class="sh">'</span><span class="p">)</span>
 <span class="n">plt</span><span class="p">.</span><span class="nf">show</span><span class="p">()</span>
</pre></td> </tr></tbody></table></code></pre></figure> <p>This will give you a plot that looks like this:</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/glm/regressors_distributions.png" sizes="95vw"></source> <img src="/assets/img/glm/regressors_distributions.png" class="img-fluid" width="700" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <h1 id="the-binomial-distribution"><a href="https://en.wikipedia.org/wiki/Binomial_distribution" rel="external nofollow noopener" target="_blank">The Binomial distribution</a></h1> <p>The response variable is binomially distributed - this means you have a discrete, non-negative number of “successes” and a discrete, non-negative number of “failures”. Here, this corresponds to <code class="language-plaintext highlighter-rouge">NABOVE</code> and <code class="language-plaintext highlighter-rouge">NBELOW</code>.</p> <h3 id="parameterization">Parameterization</h3> <p>The binomial distribution is parameterized by \(n\in \mathbb N\) and \(p\in [0,1]\), and is expressed as \(X\sim\text{Binomial}(n,p)\), where \(X\) is a binomial random variable.</p> <p>(Note: this is <em>not</em> the same \(p\) that we used earlier to define the dimensionality of each input vector \(\mathbf{x}_i ^T\)).</p> <h3 id="pmf">PMF</h3> <p>The probability mass function expresses the probability of getting \(k\) successes in \(n\) trials:</p> \[p(k,n,p)=\Pr(k;n,p)=\Pr(X=k)={n\choose k}p^k(1-p)^{n-k}\] <p>for \(k=0,1,2,...,n\) where \({n\choose k}\) is the binomial coefficient:</p> \[{n\choose k}=\frac{n!}{k!(n-k)!}\] <p>Essentially:</p> <ul> <li>\(k\) successes occur with probability \(p^k\), and</li> <li>\(n-k\) failures occur with probability \((1-p)^{n-k}\)</li> </ul> <p>Becasue the \(k\) successes can occur at any point within the \(n\) trials, there are \({n\choose k}\) different ways of observing \(k\) successes in \(n\) trials.</p> <h3 id="link-function-for-the-binomial-distribution---binary-logistic-regression">Link function for the Binomial distribution - <a href="https://newonlinecourses.science.psu.edu/stat504/node/216/" rel="external nofollow noopener" target="_blank">Binary Logistic Regression</a> </h3> <p>The Binomial GLM can be expressed as:</p> \[y_i \sim \text{Binomial}(n_i,p_i)\] <p>with the logit link function:</p> \[p_i = \text{logit} ^{-1}(\mathbf X \beta) = \frac{\exp(\mathbf X \beta)}{1+\exp(\mathbf X \beta)} = \frac{1}{1+\exp(-\mathbf X \beta)}\] <h1 id="implement-the-glm">Implement the GLM</h1> <p>We need to estimate values of \(\beta\) corresponding to each of the variables in \(\mathbf X\), to give an estimate of the effect size of each predictor - that is, a measure of the effect that they have on the binomial response variable, which in this case is <code class="language-plaintext highlighter-rouge">NABOVE</code>.</p> <p>For each sample (district) in the dataset, we have \(k_i\) (<code class="language-plaintext highlighter-rouge">NABOVE</code>) and \(n_i\) (<code class="language-plaintext highlighter-rouge">NABOVE</code>+<code class="language-plaintext highlighter-rouge">NBELOW</code>). We can estimate \(p_i\) using the link function above, as we have the values of \(\mathbf X\) from the dataset in <code class="language-plaintext highlighter-rouge">data.exog</code>. The values of \(\beta\) giving the value of \(p_i\) that defines a distribution shape that best fits to the data - comprised of \(\mathbf X\) and \(k_i =\)<code class="language-plaintext highlighter-rouge">NABOVE</code> - are to be estimated by minimizing the log-likelihood function.</p> <h3 id="likelihood-function"><a href="https://en.wikipedia.org/wiki/Likelihood_function" rel="external nofollow noopener" target="_blank">Likelihood function</a></h3> <p>We will minimize the negative log-likelihood. Here, we will use the scipy implementation of <a href="https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.binom.html" rel="external nofollow noopener" target="_blank"><code class="language-plaintext highlighter-rouge">logpmf</code></a> and the parameters \(n,k,p\) to obtain the log-likelihood. The log-likelihood function can be defined as:</p> \[\log (\mathcal{L}(n,k,p\mid \mathbf{x}))=\sum_{i=1}^{N} \log (p(x_i\mid n,k,p)) = \sum_{i=1}^{N} \log (P(X=x_i\mid n,k,p))\] <p>where,</p> <ul> <li>\(n,k,p\) are the parameters defining the shape of the binomial distribution,</li> <li>\(N\) represents the total number of school districts (303),</li> <li>\(x_i\) represents <code class="language-plaintext highlighter-rouge">NABOVE</code> for district \(i\)</li> </ul> <figure class="highlight"><pre><code class="language-python" data-lang="python"><table class="rouge-table"><tbody><tr>
<td class="gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
</pre></td> <td class="code"><pre> <span class="c1"># define the negative log-likelihood function
</span> <span class="k">def</span> <span class="nf">loglik</span><span class="p">(</span><span class="n">betas</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
     <span class="sh">'''</span><span class="s">betas = effect sizes of variables (parameters to estimate)
     x = values of variables
     y = NABOVE and NBELOW</span><span class="sh">'''</span>
    
     <span class="n">nTrials</span> <span class="o">=</span> <span class="n">y</span><span class="p">.</span><span class="n">NABOVE</span> <span class="o">+</span> <span class="n">y</span><span class="p">.</span><span class="n">NBELOW</span>
     <span class="n">pSuccess</span> <span class="o">=</span> <span class="p">(</span><span class="mi">1</span><span class="o">+</span><span class="n">np</span><span class="p">.</span><span class="nf">exp</span><span class="p">(</span><span class="o">-</span><span class="n">np</span><span class="p">.</span><span class="nf">dot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">betas</span><span class="p">)))</span><span class="o">**-</span><span class="mi">1</span>
     <span class="n">ll</span> <span class="o">=</span> <span class="n">stats</span><span class="p">.</span><span class="n">binom</span><span class="p">.</span><span class="nf">logpmf</span><span class="p">(</span><span class="n">k</span> <span class="o">=</span> <span class="n">y</span><span class="p">.</span><span class="n">NABOVE</span><span class="p">,</span> 
                             <span class="n">n</span> <span class="o">=</span> <span class="n">nTrials</span><span class="p">,</span>
                             <span class="n">p</span> <span class="o">=</span> <span class="n">pSuccess</span><span class="p">)</span>
     <span class="k">return</span> <span class="o">-</span><span class="n">ll</span><span class="p">.</span><span class="nf">sum</span><span class="p">()</span>
</pre></td> </tr></tbody></table></code></pre></figure> <h3 id="fit-model">Fit model</h3> <p>For this example, we will restrict the number of variables to just the first 5 independent variables in the STAR98 dataset. This is because the optimizer can struggle with a large parameter space (a lot of \(\beta\) values).</p> <figure class="highlight"><pre><code class="language-python" data-lang="python"><table class="rouge-table"><tbody><tr>
<td class="gutter gl"><pre class="lineno">1
2
3
4
5
</pre></td> <td class="code"><pre> <span class="c1"># fit model with first 5 variables interactions
</span> <span class="n">res_five</span> <span class="o">=</span> <span class="n">optimize</span><span class="p">.</span><span class="nf">minimize</span><span class="p">(</span><span class="n">fun</span> <span class="o">=</span> <span class="n">loglik</span><span class="p">,</span>
                             <span class="n">x0</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">zeros</span><span class="p">(</span><span class="mi">5</span><span class="p">),</span>
                             <span class="n">args</span> <span class="o">=</span> <span class="p">(</span><span class="n">data</span><span class="p">.</span><span class="n">exog</span><span class="p">.</span><span class="n">iloc</span><span class="p">[:,:</span><span class="mi">5</span><span class="p">],</span> <span class="n">data</span><span class="p">.</span><span class="n">endog</span><span class="p">),</span>
                                       <span class="n">method</span> <span class="o">=</span> <span class="sh">'</span><span class="s">Nelder-Mead</span><span class="sh">'</span><span class="p">)</span>
</pre></td> </tr></tbody></table></code></pre></figure> <p>We can see the results of the fitted model by running the command <code class="language-plaintext highlighter-rouge">res_five</code>:</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code> final_simplex: (array([[-0.00694075,  0.034228  , -0.01203383, -0.00623375, -0.00491257],
       [-0.00694209,  0.03422786, -0.01202921, -0.0062293 , -0.00491935],
       [-0.00694485,  0.03422865, -0.01202725, -0.00622885, -0.00491606],
       [-0.00694175,  0.03422388, -0.01202843, -0.0062315 , -0.00491556],
       [-0.00694161,  0.03423066, -0.01202717, -0.00623275, -0.00491647],
       [-0.00694219,  0.03422552, -0.01202785, -0.00623148, -0.0049142 ]]), array([6787.56809261, 6787.56809815, 6787.56809911, 6787.56813934,
       6787.56814474, 6787.56815288]))
           fun: 6787.568092614503
       message: 'Optimization terminated successfully.'
          nfev: 471
           nit: 286
        status: 0
       success: True
             x: array([-0.00694075,  0.034228  , -0.01203383, -0.00623375, -0.00491257])
</code></pre></div></div> <p>We can print the coefficient estimates for the first five variables in our dataset with the following command:</p> <figure class="highlight"><pre><code class="language-python" data-lang="python"><table class="rouge-table"><tbody><tr>
<td class="gutter gl"><pre class="lineno">1
2
</pre></td> <td class="code"><pre> <span class="k">for</span> <span class="n">x</span><span class="p">,</span><span class="n">y</span> <span class="ow">in</span> <span class="nf">zip</span><span class="p">(</span><span class="nf">list</span><span class="p">(</span><span class="n">data</span><span class="p">.</span><span class="n">exog</span><span class="p">)[:</span><span class="mi">5</span><span class="p">],</span> <span class="n">res_five</span><span class="p">.</span><span class="n">x</span><span class="p">):</span>
     <span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">coeff for %s = %.3g</span><span class="sh">"</span> <span class="o">%</span> <span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">y</span><span class="p">))</span>
</pre></td> </tr></tbody></table></code></pre></figure> <p>Which should return something like this:</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>coeff for LOWINC = -0.00694
coeff for PERASIAN = 0.0342
coeff for PERBLACK = -0.012
coeff for PERHISP = -0.00623
coeff for PERMINTE = -0.00491

</code></pre></div></div> <p>Let’s see what happens if we fit the model with an additional variable - that means now we use the first six independent variables instead of the first five.</p> <figure class="highlight"><pre><code class="language-python" data-lang="python"><table class="rouge-table"><tbody><tr>
<td class="gutter gl"><pre class="lineno">1
2
3
4
5
</pre></td> <td class="code"><pre> <span class="c1"># fit model with first 6 variables 
</span> <span class="n">res_six</span> <span class="o">=</span> <span class="n">optimize</span><span class="p">.</span><span class="nf">minimize</span><span class="p">(</span><span class="n">fun</span> <span class="o">=</span> <span class="n">loglik</span><span class="p">,</span>
                             <span class="n">x0</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">zeros</span><span class="p">(</span><span class="mi">6</span><span class="p">),</span>
                             <span class="n">args</span> <span class="o">=</span> <span class="p">(</span><span class="n">data</span><span class="p">.</span><span class="n">exog</span><span class="p">.</span><span class="n">iloc</span><span class="p">[:,:</span><span class="mi">6</span><span class="p">],</span> <span class="n">data</span><span class="p">.</span><span class="n">endog</span><span class="p">),</span>
                                       <span class="n">method</span><span class="o">=</span> <span class="sh">'</span><span class="s">Nelder-Mead</span><span class="sh">'</span><span class="p">)</span>
</pre></td> </tr></tbody></table></code></pre></figure> <p>You should see something like this:</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>coeff for LOWINC = -0.0159
coeff for PERASIAN = 0.0161
coeff for PERBLACK = -0.0179
coeff for PERHISP = -0.0142
coeff for PERMINTE = -0.000361
coeff for AVYRSEXP = 0.0615
</code></pre></div></div> <p>The coefficient estimates have changed for some of these variables - how can we determine which estimates are more true to the data?</p> <h1 id="does-including-another-variable-improve-the-model-fit">Does including another variable improve the model fit?</h1> <p>We can use the <a href="https://en.wikipedia.org/wiki/Likelihood-ratio_test" rel="external nofollow noopener" target="_blank"><strong>likelihood-ratio test</strong></a> to determine whether the model with six variables fits the data better than the model with five variables.</p> <p>\(H_0\): The model with six variables does not fit the data significantly better than the model with five variables.</p> <p>\(H_A\): The model with six variables fits the data significantly better than the model with five variables.</p> <p>The likelihood ratio can be computed as:</p> \[LR = -2\ln{\left( \frac{\mathcal L(\theta_0)}{\mathcal L(\theta_A)} \right)} = -2\left(\ln{\left(\mathcal L(\theta_0)\right)}-\ln{\left(\mathcal L(\theta_A)\right)}\right)\] <p>Because \(LR\) is <a href="https://en.wikipedia.org/wiki/Chi-squared_distribution" rel="external nofollow noopener" target="_blank">\(\chi^2\)-distributed</a>, we can use this property to determine the p-value of the LRT:</p> \[p = 1-\text{CDF}_k(LR)\] <p>where \(k\) is the degrees of freedom, or the difference in the number of parameters for \(H_0\) and \(H_A\). In our example, \(k = 6-5 = 1\). To compute the p-value, we will use the scipy implementation of the survival function, <a href="https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.chi2.html" rel="external nofollow noopener" target="_blank"><code class="language-plaintext highlighter-rouge">sf</code></a>.</p> <figure class="highlight"><pre><code class="language-python" data-lang="python"><table class="rouge-table"><tbody><tr>
<td class="gutter gl"><pre class="lineno">1
2
3
4
5
6
</pre></td> <td class="code"><pre><span class="c1"># define likelihood-ratio test
</span> <span class="k">def</span> <span class="nf">LRT</span><span class="p">(</span><span class="n">loglik_null</span><span class="p">,</span> <span class="n">loglik_alt</span><span class="p">,</span> <span class="n">nparams_null</span><span class="p">,</span> <span class="n">nparams_alt</span><span class="p">):</span>
     <span class="n">df</span> <span class="o">=</span> <span class="n">nparams_alt</span> <span class="o">-</span> <span class="n">nparams_null</span>
     <span class="n">lr</span> <span class="o">=</span> <span class="o">-</span><span class="mi">2</span><span class="o">*</span><span class="p">(</span><span class="n">loglik_null</span> <span class="o">-</span> <span class="n">loglik_alt</span><span class="p">)</span>
     <span class="n">p</span> <span class="o">=</span> <span class="n">stats</span><span class="p">.</span><span class="n">chi2</span><span class="p">.</span><span class="nf">sf</span><span class="p">(</span><span class="n">lr</span><span class="p">,</span> <span class="n">df</span><span class="p">)</span>
     <span class="nf">return </span><span class="p">(</span><span class="n">lr</span><span class="p">,</span> <span class="n">p</span><span class="p">)</span>
</pre></td> </tr></tbody></table></code></pre></figure> <p>Because we’ve defined the <code class="language-plaintext highlighter-rouge">LRT</code> function to take the log-likelihoods of \(H_0\) and \(H_A\), but our minimization function computed the <em>negative</em> log-likelihoods, we have to make sure to make this correction when passing those values to <code class="language-plaintext highlighter-rouge">LRT</code>.</p> <figure class="highlight"><pre><code class="language-python" data-lang="python"><table class="rouge-table"><tbody><tr>
<td class="gutter gl"><pre class="lineno">1
2
</pre></td> <td class="code"><pre> <span class="n">lr</span><span class="p">,</span> <span class="n">p</span> <span class="o">=</span> <span class="nc">LRT</span><span class="p">(</span><span class="o">-</span><span class="n">res_five</span><span class="p">.</span><span class="n">fun</span><span class="p">,</span> <span class="o">-</span><span class="n">res_six</span><span class="p">.</span><span class="n">fun</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">6</span><span class="p">)</span>
 <span class="nf">print</span><span class="p">(</span><span class="sh">'</span><span class="s">LR = %.3g, p = %.3g</span><span class="sh">'</span> <span class="o">%</span> <span class="p">(</span><span class="n">lr</span><span class="p">,</span> <span class="n">p</span><span class="p">))</span>
</pre></td> </tr></tbody></table></code></pre></figure> <p>You should get a result that looks like this:</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>LR = 6.12e+03, p = 0
</code></pre></div></div> <p>The test appears to be very significant, indicating that we reject our null hypothesis that the model using six variables does not fit the data significantly better than the model using five variables.</p> <h1 id="compare-to-statsmodels-glm">Compare to statsmodels GLM</h1> <p>Let’s use the statsmodel library’s built in GLM function to fit the GLM using the same variables that we used in our implementation and compare the results:</p> <figure class="highlight"><pre><code class="language-python" data-lang="python"><table class="rouge-table"><tbody><tr>
<td class="gutter gl"><pre class="lineno">1
2
3
4
</pre></td> <td class="code"><pre> <span class="c1"># fit binom GLM with statsmodels using first six variables
</span> <span class="n">glm_binom</span> <span class="o">=</span> <span class="n">sm</span><span class="p">.</span><span class="nc">GLM</span><span class="p">(</span><span class="n">data</span><span class="p">.</span><span class="n">endog</span><span class="p">,</span> <span class="n">data</span><span class="p">.</span><span class="n">exog</span><span class="p">.</span><span class="n">iloc</span><span class="p">[:,:</span><span class="mi">6</span><span class="p">],</span> <span class="n">family</span><span class="o">=</span><span class="n">sm</span><span class="p">.</span><span class="n">families</span><span class="p">.</span><span class="nc">Binomial</span><span class="p">())</span>
 <span class="n">res</span><span class="p">.</span><span class="n">full</span> <span class="o">=</span> <span class="n">glm_binom</span><span class="p">.</span><span class="nf">fit</span><span class="p">()</span>
 <span class="nf">print</span><span class="p">(</span><span class="n">res</span><span class="p">.</span><span class="n">full</span><span class="p">.</span><span class="nf">summary</span><span class="p">())</span>
</pre></td> </tr></tbody></table></code></pre></figure> <p>You should get something like this:</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>                  Generalized Linear Model Regression Results                   
================================================================================
Dep. Variable:     ['NABOVE', 'NBELOW']   No. Observations:                  303
Model:                              GLM   Df Residuals:                      297
Model Family:                  Binomial   Df Model:                            5
Link Function:                    logit   Scale:                          1.0000
Method:                            IRLS   Log-Likelihood:                -3727.3
Date:                  Tue, 01 Oct 2019   Deviance:                       5536.2
Time:                          13:32:36   Pearson chi2:                 5.50e+03
No. Iterations:                       4   Covariance Type:             nonrobust
==============================================================================
                 coef    std err          z      P&gt;|z|      [0.025      0.975]
------------------------------------------------------------------------------
LOWINC        -0.0159      0.000    -43.482      0.000      -0.017      -0.015
PERASIAN       0.0161      0.001     32.095      0.000       0.015       0.017
PERBLACK      -0.0179      0.001    -27.228      0.000      -0.019      -0.017
PERHISP       -0.0142      0.000    -36.839      0.000      -0.015      -0.013
PERMINTE      -0.0004      0.001     -0.672      0.502      -0.001       0.001
AVYRSEXP       0.0615      0.001     76.890      0.000       0.060       0.063
==============================================================================
</code></pre></div></div> <p>We can see that the parameter estimates for the same six variables from <code class="language-plaintext highlighter-rouge">data.exog</code> are very similar to what we got with our implementation of the GLM.</p> <p>We can also use statsmodels to fit a GLM to the entire dataset and compare the coefficient estimates to what we got fitting a model to just some of the variables in the dataset.</p> <figure class="highlight"><pre><code class="language-python" data-lang="python"><table class="rouge-table"><tbody><tr>
<td class="gutter gl"><pre class="lineno">1
2
3
4
</pre></td> <td class="code"><pre> <span class="c1"># fit binom GLM with statsmodels 
</span> <span class="n">glm_binom</span> <span class="o">=</span> <span class="n">sm</span><span class="p">.</span><span class="nc">GLM</span><span class="p">(</span><span class="n">data</span><span class="p">.</span><span class="n">endog</span><span class="p">,</span> <span class="n">data</span><span class="p">.</span><span class="n">exog</span><span class="p">,</span> <span class="n">family</span><span class="o">=</span><span class="n">sm</span><span class="p">.</span><span class="n">families</span><span class="p">.</span><span class="nc">Binomial</span><span class="p">())</span>
 <span class="n">res</span><span class="p">.</span><span class="n">full</span> <span class="o">=</span> <span class="n">glm_binom</span><span class="p">.</span><span class="nf">fit</span><span class="p">()</span>
 <span class="nf">print</span><span class="p">(</span><span class="n">res</span><span class="p">.</span><span class="n">full</span><span class="p">.</span><span class="nf">summary</span><span class="p">())</span>
</pre></td> </tr></tbody></table></code></pre></figure> <p>You should get something that looks like this:</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>                  Generalized Linear Model Regression Results                   
================================================================================
Dep. Variable:     ['NABOVE', 'NBELOW']   No. Observations:                  303
Model:                              GLM   Df Residuals:                      283
Model Family:                  Binomial   Df Model:                           19
Link Function:                    logit   Scale:                          1.0000
Method:                            IRLS   Log-Likelihood:                -3000.5
Date:                  Tue, 01 Oct 2019   Deviance:                       4082.4
Time:                          13:32:36   Pearson chi2:                 4.05e+03
No. Iterations:                       5   Covariance Type:             nonrobust
===========================================================================================
                              coef    std err          z      P&gt;|z|      [0.025      0.975]
-------------------------------------------------------------------------------------------
LOWINC                     -0.0169      0.000    -39.491      0.000      -0.018      -0.016
PERASIAN                    0.0101      0.001     16.832      0.000       0.009       0.011
PERBLACK                   -0.0186      0.001    -25.132      0.000      -0.020      -0.017
PERHISP                    -0.0142      0.000    -32.755      0.000      -0.015      -0.013
PERMINTE                    0.2778      0.027     10.154      0.000       0.224       0.331
AVYRSEXP                    0.2937      0.050      5.872      0.000       0.196       0.392
AVSALK                      0.0930      0.012      7.577      0.000       0.069       0.117
PERSPENK                   -1.4432      0.171     -8.461      0.000      -1.777      -1.109
PTRATIO                    -0.2347      0.032     -7.283      0.000      -0.298      -0.172
PCTAF                      -0.1179      0.019     -6.351      0.000      -0.154      -0.081
PCTCHRT                     0.0047      0.001      3.748      0.000       0.002       0.007
PCTYRRND                   -0.0036      0.000    -16.239      0.000      -0.004      -0.003
PERMINTE_AVYRSEXP          -0.0157      0.002     -9.142      0.000      -0.019      -0.012
PERMINTE_AVSAL             -0.0044      0.000    -10.137      0.000      -0.005      -0.004
AVYRSEXP_AVSAL             -0.0048      0.001     -5.646      0.000      -0.006      -0.003
PERSPEN_PTRATIO             0.0686      0.008      8.587      0.000       0.053       0.084
PERSPEN_PCTAF               0.0372      0.004      9.099      0.000       0.029       0.045
PTRATIO_PCTAF               0.0057      0.001      6.760      0.000       0.004       0.007
PERMINTE_AVYRSEXP_AVSAL     0.0002   2.68e-05      9.237      0.000       0.000       0.000
PERSPEN_PTRATIO_PCTAF      -0.0017      0.000     -8.680      0.000      -0.002      -0.001
===========================================================================================
</code></pre></div></div> <p>You can see that some of the coefficient estimates differ significantly from our estimates - this is due to the fact that including more variables in the model changes the coefficient estimates.</p> </div> </article> <br> <hr> <br> <ul class="list-disc pl-8"></ul> <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2> <p class="mb-2">Here are some more articles you might like to read next:</p> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2023/ols/">Ordinary least squares</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2023/mle/">Maximum likelihood estimation</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2023/gradient-descent/">Gradient descent</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2023/python_nn/">A very, very basic neural network in Python</a> </li> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2025 Jessica L. Zhou. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/assets/js/copy_code.js?12775fdf7f95e901d7119054556e495f" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>addBackToTop();</script> </body> </html>